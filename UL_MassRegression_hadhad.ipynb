{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import h5py\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import scipy as sc\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([\"fj_genMass\",\"fj_genPt\",\"fj_genEta\",\"fj_genPhi\",\"fj_n2b1\",\"fj_had1Decay\",\"fj_had2Decay\",\"fj_eventNum\",\"fj_lumiNum\",\"MET_covXX\",\"MET_covXY\",\"MET_covYY\",\"MET_phi\",\"MET_pt\",\"MET_significance\",\"PuppiMET_pt\", \"PuppiMET_phi\",\"fj_eta\",\"fj_phi\",\"fj_MuonEnergyFraction\", \"fj_ElectronEnergyFraction\", \"fj_PhotonEnergyFraction\", \"fj_ChargedHadronEnergyFraction\", \"fj_NeutralHadronEnergyFraction\", \"fj_MuonNum\", \"fj_ElectronNum\", \"fj_PhotonNum\", \"fj_ChargedHadronNum\", \"fj_NeutralHadronNum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(['fj_pt', 'fj_msd', 'fj_genMass', 'fj_genPt', 'fj_genEta', 'fj_genPhi', 'fj_n2b1', 'fj_had1Decay', 'fj_had2Decay', 'fj_eventNum', 'fj_lumiNum', 'MET_covXX', 'MET_covXY', 'MET_covYY', 'MET_phi', 'MET_pt', 'MET_significance', 'PuppiMET_pt', 'PuppiMET_phi', 'fj_eta', 'fj_phi', 'fj_MuonEnergyFraction', 'fj_ElectronEnergyFraction', 'fj_PhotonEnergyFraction', 'fj_ChargedHadronEnergyFraction', 'fj_NeutralHadronEnergyFraction', 'fj_MuonNum', 'fj_ElectronNum', 'fj_PhotonNum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.35875000e+02  1.03359375e+01  9.85000000e+01  3.87000000e+02\n",
      " -1.86328125e+00 -2.61718750e+00  3.51806641e-01  1.00000000e+00\n",
      "  1.00000000e+00  7.24000000e+02  1.80000000e+02  4.38000000e+02\n",
      " -2.55078125e+00  5.10874634e+01  3.30273438e+00  1.04129868e+02\n",
      " -2.56201172e+00 -1.95947266e+00 -2.58056641e+00  0.00000000e+00\n",
      "  0.00000000e+00  5.89671910e-01  4.11928236e-01  1.49708539e-01\n",
      "  0.00000000e+00  0.00000000e+00  7.00000000e+00  1.80000000e+01\n",
      "  1.00000000e+00  3.47412109e-01  1.98852539e-01]\n",
      "treeArrayShape\n",
      "(116198, 599)\n",
      "ColumnLen\n",
      "['fj_pt', 'fj_msd', 'fj_genMass', 'fj_genPt', 'fj_genEta', 'fj_genPhi', 'fj_n2b1', 'fj_had1Decay', 'fj_had2Decay', 'fj_eventNum', 'fj_lumiNum', 'MET_covXX', 'MET_covXY', 'MET_covYY', 'MET_phi', 'MET_pt', 'MET_significance', 'PuppiMET_pt', 'PuppiMET_phi', 'fj_eta', 'fj_phi', 'fj_MuonEnergyFraction', 'fj_ElectronEnergyFraction', 'fj_PhotonEnergyFraction', 'fj_ChargedHadronEnergyFraction', 'fj_NeutralHadronEnergyFraction', 'fj_MuonNum', 'fj_ElectronNum', 'fj_PhotonNum', 'PF_pt0', 'PF_pt1', 'PF_pt2', 'PF_pt3', 'PF_pt4', 'PF_pt5', 'PF_pt6', 'PF_pt7', 'PF_pt8', 'PF_pt9', 'PF_pt10', 'PF_pt11', 'PF_pt12', 'PF_pt13', 'PF_pt14', 'PF_pt15', 'PF_pt16', 'PF_pt17', 'PF_pt18', 'PF_pt19', 'PF_pt20', 'PF_pt21', 'PF_pt22', 'PF_pt23', 'PF_pt24', 'PF_pt25', 'PF_pt26', 'PF_pt27', 'PF_pt28', 'PF_pt29', 'PF_eta0', 'PF_eta1', 'PF_eta2', 'PF_eta3', 'PF_eta4', 'PF_eta5', 'PF_eta6', 'PF_eta7', 'PF_eta8', 'PF_eta9', 'PF_eta10', 'PF_eta11', 'PF_eta12', 'PF_eta13', 'PF_eta14', 'PF_eta15', 'PF_eta16', 'PF_eta17', 'PF_eta18', 'PF_eta19', 'PF_eta20', 'PF_eta21', 'PF_eta22', 'PF_eta23', 'PF_eta24', 'PF_eta25', 'PF_eta26', 'PF_eta27', 'PF_eta28', 'PF_eta29', 'PF_phi0', 'PF_phi1', 'PF_phi2', 'PF_phi3', 'PF_phi4', 'PF_phi5', 'PF_phi6', 'PF_phi7', 'PF_phi8', 'PF_phi9', 'PF_phi10', 'PF_phi11', 'PF_phi12', 'PF_phi13', 'PF_phi14', 'PF_phi15', 'PF_phi16', 'PF_phi17', 'PF_phi18', 'PF_phi19', 'PF_phi20', 'PF_phi21', 'PF_phi22', 'PF_phi23', 'PF_phi24', 'PF_phi25', 'PF_phi26', 'PF_phi27', 'PF_phi28', 'PF_phi29', 'PF_q0', 'PF_q1', 'PF_q2', 'PF_q3', 'PF_q4', 'PF_q5', 'PF_q6', 'PF_q7', 'PF_q8', 'PF_q9', 'PF_q10', 'PF_q11', 'PF_q12', 'PF_q13', 'PF_q14', 'PF_q15', 'PF_q16', 'PF_q17', 'PF_q18', 'PF_q19', 'PF_q20', 'PF_q21', 'PF_q22', 'PF_q23', 'PF_q24', 'PF_q25', 'PF_q26', 'PF_q27', 'PF_q28', 'PF_q29', 'PF_dz0', 'PF_dz1', 'PF_dz2', 'PF_dz3', 'PF_dz4', 'PF_dz5', 'PF_dz6', 'PF_dz7', 'PF_dz8', 'PF_dz9', 'PF_dz10', 'PF_dz11', 'PF_dz12', 'PF_dz13', 'PF_dz14', 'PF_dz15', 'PF_dz16', 'PF_dz17', 'PF_dz18', 'PF_dz19', 'PF_dz20', 'PF_dz21', 'PF_dz22', 'PF_dz23', 'PF_dz24', 'PF_dz25', 'PF_dz26', 'PF_dz27', 'PF_dz28', 'PF_dz29', 'PF_dzerr0', 'PF_dzerr1', 'PF_dzerr2', 'PF_dzerr3', 'PF_dzerr4', 'PF_dzerr5', 'PF_dzerr6', 'PF_dzerr7', 'PF_dzerr8', 'PF_dzerr9', 'PF_dzerr10', 'PF_dzerr11', 'PF_dzerr12', 'PF_dzerr13', 'PF_dzerr14', 'PF_dzerr15', 'PF_dzerr16', 'PF_dzerr17', 'PF_dzerr18', 'PF_dzerr19', 'PF_dzerr20', 'PF_dzerr21', 'PF_dzerr22', 'PF_dzerr23', 'PF_dzerr24', 'PF_dzerr25', 'PF_dzerr26', 'PF_dzerr27', 'PF_dzerr28', 'PF_dzerr29', 'PF_dxy0', 'PF_dxy1', 'PF_dxy2', 'PF_dxy3', 'PF_dxy4', 'PF_dxy5', 'PF_dxy6', 'PF_dxy7', 'PF_dxy8', 'PF_dxy9', 'PF_dxy10', 'PF_dxy11', 'PF_dxy12', 'PF_dxy13', 'PF_dxy14', 'PF_dxy15', 'PF_dxy16', 'PF_dxy17', 'PF_dxy18', 'PF_dxy19', 'PF_dxy20', 'PF_dxy21', 'PF_dxy22', 'PF_dxy23', 'PF_dxy24', 'PF_dxy25', 'PF_dxy26', 'PF_dxy27', 'PF_dxy28', 'PF_dxy29', 'PF_dxyerr0', 'PF_dxyerr1', 'PF_dxyerr2', 'PF_dxyerr3', 'PF_dxyerr4', 'PF_dxyerr5', 'PF_dxyerr6', 'PF_dxyerr7', 'PF_dxyerr8', 'PF_dxyerr9', 'PF_dxyerr10', 'PF_dxyerr11', 'PF_dxyerr12', 'PF_dxyerr13', 'PF_dxyerr14', 'PF_dxyerr15', 'PF_dxyerr16', 'PF_dxyerr17', 'PF_dxyerr18', 'PF_dxyerr19', 'PF_dxyerr20', 'PF_dxyerr21', 'PF_dxyerr22', 'PF_dxyerr23', 'PF_dxyerr24', 'PF_dxyerr25', 'PF_dxyerr26', 'PF_dxyerr27', 'PF_dxyerr28', 'PF_dxyerr29', 'PF_pup0', 'PF_pup1', 'PF_pup2', 'PF_pup3', 'PF_pup4', 'PF_pup5', 'PF_pup6', 'PF_pup7', 'PF_pup8', 'PF_pup9', 'PF_pup10', 'PF_pup11', 'PF_pup12', 'PF_pup13', 'PF_pup14', 'PF_pup15', 'PF_pup16', 'PF_pup17', 'PF_pup18', 'PF_pup19', 'PF_pup20', 'PF_pup21', 'PF_pup22', 'PF_pup23', 'PF_pup24', 'PF_pup25', 'PF_pup26', 'PF_pup27', 'PF_pup28', 'PF_pup29', 'PF_pupnolep0', 'PF_pupnolep1', 'PF_pupnolep2', 'PF_pupnolep3', 'PF_pupnolep4', 'PF_pupnolep5', 'PF_pupnolep6', 'PF_pupnolep7', 'PF_pupnolep8', 'PF_pupnolep9', 'PF_pupnolep10', 'PF_pupnolep11', 'PF_pupnolep12', 'PF_pupnolep13', 'PF_pupnolep14', 'PF_pupnolep15', 'PF_pupnolep16', 'PF_pupnolep17', 'PF_pupnolep18', 'PF_pupnolep19', 'PF_pupnolep20', 'PF_pupnolep21', 'PF_pupnolep22', 'PF_pupnolep23', 'PF_pupnolep24', 'PF_pupnolep25', 'PF_pupnolep26', 'PF_pupnolep27', 'PF_pupnolep28', 'PF_pupnolep29', 'PF_id0', 'PF_id1', 'PF_id2', 'PF_id3', 'PF_id4', 'PF_id5', 'PF_id6', 'PF_id7', 'PF_id8', 'PF_id9', 'PF_id10', 'PF_id11', 'PF_id12', 'PF_id13', 'PF_id14', 'PF_id15', 'PF_id16', 'PF_id17', 'PF_id18', 'PF_id19', 'PF_id20', 'PF_id21', 'PF_id22', 'PF_id23', 'PF_id24', 'PF_id25', 'PF_id26', 'PF_id27', 'PF_id28', 'PF_id29', 'PF_trk0', 'PF_trk1', 'PF_trk2', 'PF_trk3', 'PF_trk4', 'PF_trk5', 'PF_trk6', 'PF_trk7', 'PF_trk8', 'PF_trk9', 'PF_trk10', 'PF_trk11', 'PF_trk12', 'PF_trk13', 'PF_trk14', 'PF_trk15', 'PF_trk16', 'PF_trk17', 'PF_trk18', 'PF_trk19', 'PF_trk20', 'PF_trk21', 'PF_trk22', 'PF_trk23', 'PF_trk24', 'PF_trk25', 'PF_trk26', 'PF_trk27', 'PF_trk28', 'PF_trk29', 'PF_vtx0', 'PF_vtx1', 'PF_vtx2', 'PF_vtx3', 'PF_vtx4', 'PF_vtx5', 'PF_vtx6', 'PF_vtx7', 'PF_vtx8', 'PF_vtx9', 'PF_vtx10', 'PF_vtx11', 'PF_vtx12', 'PF_vtx13', 'PF_vtx14', 'PF_vtx15', 'PF_vtx16', 'PF_vtx17', 'PF_vtx18', 'PF_vtx19', 'PF_vtx20', 'PF_vtx21', 'PF_vtx22', 'PF_vtx23', 'PF_vtx24', 'PF_vtx25', 'PF_vtx26', 'PF_vtx27', 'PF_vtx28', 'PF_vtx29', 'sv_dlen0', 'sv_dlen1', 'sv_dlen2', 'sv_dlen3', 'sv_dlen4', 'sv_dlenSig0', 'sv_dlenSig1', 'sv_dlenSig2', 'sv_dlenSig3', 'sv_dlenSig4', 'sv_dxy0', 'sv_dxy1', 'sv_dxy2', 'sv_dxy3', 'sv_dxy4', 'sv_dxySig0', 'sv_dxySig1', 'sv_dxySig2', 'sv_dxySig3', 'sv_dxySig4', 'sv_chi20', 'sv_chi21', 'sv_chi22', 'sv_chi23', 'sv_chi24', 'sv_pAngle0', 'sv_pAngle1', 'sv_pAngle2', 'sv_pAngle3', 'sv_pAngle4', 'sv_x0', 'sv_x1', 'sv_x2', 'sv_x3', 'sv_x4', 'sv_y0', 'sv_y1', 'sv_y2', 'sv_y3', 'sv_y4', 'sv_z0', 'sv_z1', 'sv_z2', 'sv_z3', 'sv_z4', 'sv_pt0', 'sv_pt1', 'sv_pt2', 'sv_pt3', 'sv_pt4', 'sv_mass0', 'sv_mass1', 'sv_mass2', 'sv_mass3', 'sv_mass4', 'sv_eta0', 'sv_eta1', 'sv_eta2', 'sv_eta3', 'sv_eta4', 'sv_phi0', 'sv_phi1', 'sv_phi2', 'sv_phi3', 'sv_phi4', 'tau_charge0', 'tau_charge1', 'tau_charge2', 'tau_chargedIso0', 'tau_chargedIso1', 'tau_chargedIso2', 'tau_eta0', 'tau_eta1', 'tau_eta2', 'tau_leadTkDeltaEta0', 'tau_leadTkDeltaEta1', 'tau_leadTkDeltaEta2', 'tau_leadTkDeltaPhi0', 'tau_leadTkDeltaPhi1', 'tau_leadTkDeltaPhi2', 'tau_leadTkPtOverTauPt0', 'tau_leadTkPtOverTauPt1', 'tau_leadTkPtOverTauPt2', 'tau_mass0', 'tau_mass1', 'tau_mass2', 'tau_neutralIso0', 'tau_neutralIso1', 'tau_neutralIso2', 'tau_phi0', 'tau_phi1', 'tau_phi2', 'tau_photonsOutsideSignalCone0', 'tau_photonsOutsideSignalCone1', 'tau_photonsOutsideSignalCone2', 'tau_pt0', 'tau_pt1', 'tau_pt2', 'tau_rawAntiEle20180', 'tau_rawAntiEle20181', 'tau_rawAntiEle20182', 'tau_rawIso0', 'tau_rawIso1', 'tau_rawIso2', 'tau_rawIsodR030', 'tau_rawIsodR031', 'tau_rawIsodR032', 'electron_charge0', 'electron_charge1', 'electron_convVeto0', 'electron_convVeto1', 'electron_deltaEtaSC0', 'electron_deltaEtaSC1', 'electron_dr03EcalRecHitSumEt0', 'electron_dr03EcalRecHitSumEt1', 'electron_dr03HcalDepth1TowerSumEt0', 'electron_dr03HcalDepth1TowerSumEt1', 'electron_dr03TkSumPt0', 'electron_dr03TkSumPt1', 'electron_dxy0', 'electron_dxy1', 'electron_dxyErr0', 'electron_dxyErr1', 'electron_dz0', 'electron_dz1', 'electron_dzErr0', 'electron_dzErr1', 'electron_eInvMinusPInv0', 'electron_eInvMinusPInv1', 'electron_eta0', 'electron_eta1', 'electron_hoe0', 'electron_hoe1', 'electron_ip3d0', 'electron_ip3d1', 'electron_lostHits0', 'electron_lostHits1', 'electron_phi0', 'electron_phi1', 'electron_pt0', 'electron_pt1', 'electron_r90', 'electron_r91', 'electron_sieie0', 'electron_sieie1', 'electron_sip3d0', 'electron_sip3d1', 'muon_charge0', 'muon_charge1', 'muon_dxy0', 'muon_dxy1', 'muon_dxyErr0', 'muon_dxyErr1', 'muon_dz0', 'muon_dz1', 'muon_dzErr0', 'muon_dzErr1', 'muon_eta0', 'muon_eta1', 'muon_ip3d0', 'muon_ip3d1', 'muon_nStations0', 'muon_nStations1', 'muon_nTrackerLayers0', 'muon_nTrackerLayers1', 'muon_pfRelIso03_all0', 'muon_pfRelIso03_all1', 'muon_pfRelIso03_chg0', 'muon_pfRelIso03_chg1', 'muon_phi0', 'muon_phi1', 'muon_pt0', 'muon_pt1', 'muon_segmentComp0', 'muon_segmentComp1', 'muon_sip3d0', 'muon_sip3d1', 'muon_tkRelIso0', 'muon_tkRelIso1', 'fj_decayType']\n",
      "599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PF_dzerr0 found nan!!\n",
      "PF_dzerr1 found nan!!\n",
      "PF_dzerr2 found nan!!\n",
      "PF_dzerr3 found nan!!\n",
      "PF_dzerr4 found nan!!\n",
      "PF_dzerr5 found nan!!\n",
      "PF_dzerr7 found nan!!\n",
      "PF_dzerr9 found nan!!\n",
      "(116137, 390)\n",
      "(116137, 30, 13)\n",
      "(116137, 65)\n",
      "(116137, 5, 13)\n",
      "(116137, 12)\n"
     ]
    }
   ],
   "source": [
    "from preprocessor import load, load_flat\n",
    "\n",
    "X_z,Xalt_z, Xevt_z, y_z, feat_z = load_flat('/uscms_data/d3/keiran/CMSSW_10_2_11/src/PandaAnalysis/dazsle-tagger/evt/massReg_UL_hadhad/Apr21/tt-DYJetsToLL_noLep.z');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Add, Concatenate, BatchNormalization, Conv1D, Lambda, Dot, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import load, load_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_htt,X_test_htt,Xalt_train_htt,Xalt_test_htt,Xevt_train_htt,Xevt_test_htt,Y_train_htt,Y_test_htt,feat_train_htt,feat_test_htt = load('/nobackup/users/sangeon/GluGluHToTauTau_user.z');\n",
    "#X_train_flat,X_test_flat,Xalt_train_flat,Xalt_test_flat,Xevt_train_flat,Xevt_test_flat,Y_train_flat,Y_test_flat,feat_train_flat,feat_test_flat = load('/nobackup/users/sangeon/FlatTauTau_user.z',maxevts=0);\n",
    "#X_train_z,X_test_z,Xalt_train_z,Xalt_test_z,Xevt_train_z,Xevt_test_z,Y_train_z,Y_test_z,feat_train_z,feat_test_z = load('/nobackup/users/sangeon/DYJetsToLL.z');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selection = np.r_[np.array([0,1,2,3,4]), np.array([6,7,8,9,10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import load, load_flat\n",
    "X_flat,Xalt_flat,Xevt_flat,y_flat,feat_flat = load_flat('/uscms_data/d3/keiran/CMSSW_10_2_11/src/PandaAnalysis/dazsle-tagger/evt/hadhad_UL/Apr21/FlatTauTau_user_noLep.z',maxevts=0);\n",
    "#X_htt1,Xalt_htt1,Xevt_htt1,y_htt1,feat_htt1 = load_flat('/nobackup/users/sangeon/phitautau_UL/GluGluHToTauTau_user_noLep.z'); \n",
    "X_htt2,Xalt_htt2,Xevt_htt2,y_htt2,feat_htt2 = load_flat('/uscms_data/d3/keiran/CMSSW_10_2_11/src/PandaAnalysis/dazsle-tagger/evt/massReg_UL/Feb24/GluGluHToTauTau_user_noLep.z');\n",
    "X_z,Xalt_z, Xevt_z, y_z, feat_z = load_flat('/uscms_data/d3/keiran/CMSSW_10_2_11/src/PandaAnalysis/dazsle-tagger/evt/massReg_UL/Feb20/tt-DYJetsToLL_noLep.z');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_htt = np.vstack([X_htt2])\n",
    "\n",
    "Xalt_htt = np.vstack([Xalt_htt2])\n",
    "\n",
    "Xevt_htt = np.vstack([Xevt_htt2])\n",
    "\n",
    "y_htt = np.vstack([y_htt2])\n",
    "\n",
    "feat_htt = np.vstack([feat_htt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_htt = np.vstack([X_htt1, X_htt2])\n",
    "\n",
    "#Xalt_htt = np.vstack([Xalt_htt1, Xalt_htt2])\n",
    "\n",
    "#Xevt_htt = np.vstack([Xevt_htt1, Xevt_htt2])\n",
    "\n",
    "#y_htt = np.vstack([y_htt1, y_htt2])\n",
    "\n",
    "#feat_htt = np.vstack([feat_htt1, feat_htt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_htt.shape, X_z.shape, X_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#met cut > 40 GeV\n",
    "\n",
    "#Z tagger 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_htt = X_htt[:,:,feat_selection]\n",
    "#X_z = X_z[:,:,feat_selection]\n",
    "#X_flat = X_flat[:,:,feat_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"/uscms_data/d1/eamoreno/Analysis/phitautau/MassRegression/figs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import AnchoredText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Xevt_flat[:,-3],bins=np.arange(0,500,1));\n",
    "\n",
    "summary = f'''mean:{np.mean(Xevt_flat[:,-3])}\n",
    "std:{np.std(Xevt_flat[:,-3])}'''\n",
    "\n",
    "anchored_text = AnchoredText(summary, loc=1)\n",
    "#anchored_text2 = AnchoredText(f\"std:{np.std(Xevt_flat[:,-4])}\", loc=4)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(anchored_text)\n",
    "#ax.add_artist(anchored_text2)\n",
    "plt.savefig(prefix+'flat_msd.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_htt[:,-4],bins=np.arange(0,300,1));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"fj_genMass\",\"fj_genPt\",\"fj_genEta\",\"fj_genPhi\"]\n",
    "target_old = \"fj_msd\"\n",
    "#target_norm = \"fj_msd\"\n",
    "target_norm = \"\"\n",
    "evt_feats = [\"MET_covXX\",\"MET_covXY\",\"MET_covYY\",\"MET_phi\",\"MET_pt\",\"MET_significance\",\"PuppiMET_pt\",\"PuppiMET_phi\",\"fj_msd\",\"fj_pt\",\"fj_eta\",\"fj_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Xevt_z[:,-4],bins=np.arange(0,300,1));\n",
    "plt.savefig(prefix+'z_msd.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_msd_cut = np.where((Xevt_flat[:,-4]>10)&(Xevt_flat[:,-4]<400)&(Xevt_flat[:,4]>40))\n",
    "htt_msd_cut = np.where((Xevt_htt[:,-4]>10)&(Xevt_htt[:,-4]<400)&(Xevt_htt[:,4]>40))\n",
    "z_msd_cut = np.where((Xevt_z[:,-4]>10)&(Xevt_z[:,-4]<400)&(y_z[:,-4]>5)&(Xevt_z[:,4]>40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_msd_cut = np.where((Xevt_flat[:,-4]>20)&(Xevt_flat[:,-3]>200)&(y_flat[:,1]>400))\n",
    "#htt_msd_cut = np.where((Xevt_htt[:,-4]>20)&(Xevt_htt[:,-3]>200)&(y_htt[:,1]>400))\n",
    "#z_msd_cut = np.where((Xevt_z[:,-4]>20)&(Xevt_z[:,-3]>200)&(y_z[:,1]>400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat,Xalt_flat,Xevt_flat,y_flat,feat_flat = X_flat[flat_msd_cut], Xalt_flat[flat_msd_cut], Xevt_flat[flat_msd_cut], y_flat[flat_msd_cut], feat_flat[flat_msd_cut]\n",
    "\n",
    "X_htt,Xalt_htt,Xevt_htt,y_htt,feat_htt = X_htt[htt_msd_cut], Xalt_htt[htt_msd_cut], Xevt_htt[htt_msd_cut], y_htt[htt_msd_cut], feat_htt[htt_msd_cut]\n",
    "\n",
    "X_z,Xalt_z,Xevt_z,y_z,feat_z = X_z[z_msd_cut], Xalt_z[z_msd_cut], Xevt_z[z_msd_cut], y_z[z_msd_cut], feat_z[z_msd_cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_flat.shape, X_htt.shape, X_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_z[:,-4],bins=np.arange(0,1000,2));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Xevt_flat[:,-4],bins=np.arange(0,1000,2));\n",
    "#plt.savefig(prefix+'flat_msd_after_cut.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_htt, Xalt_htt, Xevt_htt, y_htt = sklearn.utils.shuffle(X_htt, Xalt_htt, Xevt_htt, y_htt)\n",
    "X_z, Xalt_z, Xevt_z, y_z = sklearn.utils.shuffle(X_z, Xalt_z, Xevt_z, y_z)\n",
    "X_flat, Xalt_flat, Xevt_flat, y_flat = sklearn.utils.shuffle(X_flat, Xalt_flat, Xevt_flat, y_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Mixture Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wp_list = [0.2, 0.5, 0.8]\n",
    "#wp_choice = 1\n",
    "#wp = wp_list[wp_choice]\n",
    "#print(wp)\n",
    "#X_train_split1_htt, X_train_split2_htt, Xalt_train_split1_htt, Xalt_train_split2_htt, Xevt_train_split1_htt, Xevt_train_split2_htt, y_train_split1_htt, y_train_split2_htt = train_test_split(X_train_htt, Xalt_train_htt, Xevt_train_htt, Y_train_htt, test_size=wp, random_state=42)\n",
    "#X_train_split1_z, X_train_split2_z, Xalt_train_split1_z, Xalt_train_split2_z, Xevt_train_split1_z, Xevt_train_split2_z, y_train_split1_z, y_train_split2_z = train_test_split(X_train_z, Xalt_train_z, Xevt_train_z, Y_train_z, test_size=wp, random_state=42)\n",
    "#mixratio = round(X_train_split1_htt.shape[0]/(X_train_flat.shape[0]+X_train_split1_htt.shape[0])*100)\n",
    "#print(mixratio)\n",
    "#X_train = X_train_flat\n",
    "#Xalt_train = Xalt_train_flat\n",
    "#Xevt_train = Xevt_train_flat\n",
    "#y_train = (Y_train_flat[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.vstack((X_train_split1_htt, X_train_split1_z, X_train_flat))\n",
    "#Xalt_train = np.vstack((Xalt_train_split1_htt, Xalt_train_split1_z, Xalt_train_flat))\n",
    "#Xevt_train = np.vstack((Xevt_train_split1_htt, Xevt_train_split1_z, Xevt_train_flat))\n",
    "#y_train = np.concatenate((y_train_split1_htt[:,0], y_train_split1_z[:,0], Y_train_flat[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val = np.vstack((X_train_split2_htt, X_train_split2_z,X_test_flat))\n",
    "#Xalt_val = np.vstack((Xalt_train_split2_htt, Xalt_train_split2_z,Xalt_test_flat))\n",
    "#Xevt_val = np.vstack((Xevt_train_split2_htt, Xevt_train_split2_z,Xevt_test_flat))\n",
    "#y_val = np.concatenate((y_train_split2_htt[:,0], y_train_split2_z[:,0], Y_test_flat[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particlesConsidered = 30\n",
    "entriesPerParticle = 13\n",
    "\n",
    "svConsidered = 5\n",
    "entriesPerSV = 13\n",
    "\n",
    "eventDataLength = 12\n",
    "\n",
    "#decayTypeColumn = 380\n",
    "#trainingDataLength = 1070000\n",
    "#validationDataLength = 130000\n",
    "\n",
    "numberOfEpochs = 5\n",
    "batchSize = 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## receiving matrix\n",
    "\n",
    "RR = []\n",
    "for i in range(particlesConsidered):\n",
    "    row = []\n",
    "    for j in range(particlesConsidered * (particlesConsidered - 1)):\n",
    "        if j in range(i * (particlesConsidered - 1), (i + 1) * (particlesConsidered - 1)):\n",
    "            row.append(1.0)\n",
    "        else:\n",
    "            row.append(0.0)\n",
    "    RR.append(row)\n",
    "RR = np.array(RR)\n",
    "RR = np.float32(RR)\n",
    "RRT = np.transpose(RR)\n",
    "\n",
    "## sending matrix\n",
    "\n",
    "RST = []\n",
    "for i in range(particlesConsidered):\n",
    "    for j in range(particlesConsidered):\n",
    "        row = []\n",
    "        for k in range(particlesConsidered):\n",
    "            if k == j:\n",
    "                row.append(1.0)\n",
    "            else:\n",
    "                row.append(0.0)\n",
    "        RST.append(row)\n",
    "rowsToRemove = []\n",
    "for i in range(particlesConsidered):\n",
    "    rowsToRemove.append(i * (particlesConsidered + 1))\n",
    "RST = np.array(RST)\n",
    "RST = np.float32(RST)\n",
    "RST = np.delete(RST, rowsToRemove, 0)\n",
    "RS = np.transpose(RST)\n",
    "\n",
    "## recieving matrix for the bipartite particle and secondary vertex graph\n",
    "\n",
    "RK = []\n",
    "for i in range(particlesConsidered):\n",
    "    row = []\n",
    "    for j in range(particlesConsidered * svConsidered):\n",
    "        if j in range(i * svConsidered, (i + 1) * svConsidered):\n",
    "            row.append(1.0)\n",
    "        else:\n",
    "            row.append(0.0)\n",
    "    RK.append(row)\n",
    "RK = np.array(RK)\n",
    "RK = np.float32(RK)\n",
    "RKT = np.transpose(RK)\n",
    "\n",
    "## defines the sending matrix for the bipartite particle and secondary vertex graph\n",
    "\n",
    "\n",
    "RV = []\n",
    "for i in range(svConsidered):\n",
    "    row = []\n",
    "    for j in range(particlesConsidered * svConsidered):\n",
    "        if j % svConsidered == i:\n",
    "            row.append(1.0)\n",
    "        else:\n",
    "            row.append(0.0)\n",
    "    RV.append(row)\n",
    "RV = np.array(RV)\n",
    "RV = np.float32(RV)\n",
    "RVT = np.transpose(RV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputParticle = Input(shape=(particlesConsidered, entriesPerParticle), name=\"inputParticle\")\n",
    "\n",
    "XdotRR = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RR, axes=[[2], [0]]),\n",
    "                                            perm=(0, 2, 1)), name=\"XdotRR\")(inputParticle)\n",
    "XdotRS = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RS, axes=[[2], [0]]),\n",
    "                                            perm=(0, 2, 1)), name=\"XdotRS\")(inputParticle)\n",
    "Bpp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bpp\")([XdotRR, XdotRS])\n",
    "\n",
    "convOneParticle = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneParticle\")(Bpp)\n",
    "convTwoParticle = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoParticle\")(convOneParticle)\n",
    "convThreeParticle = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeParticle\")(convTwoParticle)\n",
    "\n",
    "Epp = BatchNormalization(momentum=0.6, name=\"Epp\")(convThreeParticle)\n",
    "\n",
    "# Secondary vertex data interaction NN\n",
    "inputSV = Input(shape=(svConsidered, entriesPerSV), name=\"inputSV\")\n",
    "\n",
    "XdotRK = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RK, axes=[[2], [0]]),\n",
    "                                            perm=(0, 2, 1)), name=\"XdotRK\")(inputParticle)\n",
    "YdotRV = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RV, axes=[[2], [0]]),\n",
    "                                            perm=(0, 2, 1)), name=\"YdotRV\")(inputSV)\n",
    "Bvp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bvp\")([XdotRK, YdotRV])\n",
    "\n",
    "convOneSV = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneSV\")(Bvp)\n",
    "convTwoSV = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoSV\")(convOneSV)\n",
    "convThreeSV = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeSV\")(convTwoSV)\n",
    "\n",
    "Evp = BatchNormalization(momentum=0.6, name=\"Evp\")(convThreeSV)\n",
    "\n",
    "\n",
    "# Event Level Info\n",
    "\n",
    "inputEvent = Input(shape=(eventDataLength, ), name=\"inputEvent\")\n",
    "\n",
    "\n",
    "# Combined prediction NN\n",
    "EppBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RRT, axes=[[2], [0]]),\n",
    "                                            perm=(0, 2, 1)), name=\"EppBar\")(Epp)\n",
    "EvpBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RKT, axes=[[2], [0]]),\n",
    "                                            perm=(0, 2, 1)), name=\"EvpBar\")(Evp)\n",
    "C = Lambda(lambda listOfTensors: tf.concat((listOfTensors[0], listOfTensors[1], listOfTensors[2]), axis=2), name=\"C\")(\n",
    "    [inputParticle, EppBar, EvpBar])\n",
    "\n",
    "convPredictOne = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convPredictOne\")(C)\n",
    "convPredictTwo = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convPredictTwo\")(convPredictOne)\n",
    "\n",
    "O = Conv1D(24, kernel_size=1, activation=\"relu\", name=\"O\")(convPredictTwo)\n",
    "\n",
    "# Calculate output\n",
    "OBar = Lambda(lambda tensor: K.sum(tensor, axis=1), name=\"OBar\")(O)\n",
    "\n",
    "Concatted = Concatenate()([OBar, inputEvent])\n",
    "\n",
    "denseEndOne = Dense(50, activation=\"relu\", name=\"denseEndOne\")(Concatted)\n",
    "normEndOne = BatchNormalization(momentum=0.6, name=\"normEndOne\")(denseEndOne)\n",
    "denseEndTwo = Dense(20, activation=\"relu\", name=\"denseEndTwo\")(normEndOne)\n",
    "denseEndThree = Dense(10, activation=\"relu\", name=\"denseEndThree\")(denseEndTwo)\n",
    "output = Dense(2,name=\"output\")(denseEndThree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_flat.shape, X_htt.shape, X_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_H = [12000,15000,20000,25000,30000] #12000 done\n",
    "#NUM_Z = [12000,15000,20000,25000,30000]\n",
    "\n",
    "#NUM_H = [12000] #12000 done\n",
    "#NUM_Z = [20000,25000,30000]\n",
    "\n",
    "#NUM_H = [15000]\n",
    "#NUM_Z = [12000,15000,20000,25000,30000]\n",
    "\n",
    "#NUM_H = [15000]\n",
    "#NUM_Z = [25000,30000]\n",
    "\n",
    "#NUM_H = [20000]\n",
    "#NUM_Z = [12000,15000,20000,25000,30000]\n",
    "\n",
    "#NUM_H = [25000]\n",
    "#NUM_Z = [12000,15000,20000]\n",
    "\n",
    "#NUM_H = [25000,30000]\n",
    "#NUM_Z = [25000,30000]\n",
    "\n",
    "#NUM_H = [30000]\n",
    "#NUM_Z = [25000,30000]\n",
    "\n",
    "NUM_H = [50000]\n",
    "NUM_Z = [50000]\n",
    "\n",
    "#NUM_H = [100000]\n",
    "#NUM_Z = [100000]\n",
    "#NUM_H = [2000,4000]\n",
    "#NUM_Z = [2000,4000]\n",
    "#NUM_H = [6000,7000,8000]\n",
    "#NUM_Z = [6000,7000,8000]\n",
    "HOW_MANY_FLAT_TO_TRAINING = 0\n",
    "#HOW_MANY_H_Z_TO_VALIDATION = 200000\n",
    "#HOW_MANY_FLAT_TO_VALIDATION = 200000\n",
    "#HOW_MANY_H_Z_TO_VALIDATION = 100000\n",
    "#HOW_MANY_FLAT_TO_VALIDATION = 100000\n",
    "TEST_SIZE = 10000\n",
    "#lamb_list = [0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "lamb_list = [0.005, 0.01, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "prefix = \"/uscms_data/d1/eamoreno/Analysis/phitautau/MassRegression/figs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_htt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([[1,2,3,4],[5,6,7,8]])\n",
    "print(a.shape)\n",
    "print(np.mean(a,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sqrt_error(y_true, y_pred):\n",
    "    diff = math_ops.abs((y_true - y_pred) / K.maximum(tf.sqrt(y_true), K.epsilon()))\n",
    "    return diff[...,0]+lamb*diff[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_qcd,Xalt_qcd,Xevt_qcd,y_qcd,feat_qcd = load_flat('/uscms_data/d3/keiran/CMSSW_10_2_11/src/PandaAnalysis/dazsle-tagger/evt/hadhad_UL/Apr21/QCD_noLep.z',maxevts=10000);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_msd_cut = np.where((Xevt_qcd[:,-4]>10)&(Xevt_qcd[:,-4]<400))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_qcd,Xalt_qcd,Xevt_qcd,y_qcd,feat_qcd = X_qcd[qcd_msd_cut], Xalt_qcd[qcd_msd_cut], Xevt_qcd[qcd_msd_cut], y_qcd[qcd_msd_cut], feat_qcd[qcd_msd_cut]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_qcd = X_qcd[:,:,feat_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#prefix='/nobackup/users/sangeon/figures/finalPlots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wps = [(15000,20000,0.1)]\n",
    "wps = [(20000,25000,0.01)]\n",
    "#wps = [(0,30000,0.01)]\n",
    "#wps = [(10000,10000,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(10000,10000,0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(5000,5000,0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(7000,7000,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(9000,16000,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(0,0,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(1000,0,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(500,100,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(1000,500,.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(5000,5000,.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(1000,1000,.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_htt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 50000\n",
    "#lamb_list = [0.005, 0.01, 0.05]\n",
    "HOW_MANY_FLAT_TO_TRAINING = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 10000\n",
    "#lamb_list = [0.005, 0.01, 0.05]\n",
    "HOW_MANY_FLAT_TO_TRAINING = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model, x_test, y_test):\n",
    "        self.model  = model\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #print(type(self.validation_data))\n",
    "        y_pred = self.model.predict(self.x_test)\n",
    "        print('prediction: {} at epoch: {}'.format(y_pred, epoch))\n",
    "        print('prediction Truth: {} at epoch: {}'.format(self.y_test, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(2000,6000,0.01),(2000,8000,0.01),(2000,10000,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = [(1800,6000,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = './figs/hadhad/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_htt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for HOW_MANY_H_TO_TRAINING, HOW_MANY_Z_TO_TRAINING, lamb in wps:\n",
    "\n",
    "            modelName=f\"NEW_UL_NoFeatSel_H{HOW_MANY_H_TO_TRAINING}_Z{HOW_MANY_Z_TO_TRAINING}_FLAT{HOW_MANY_FLAT_TO_TRAINING}_Lambda{lamb}_hadhad\"\n",
    "            #modelName=f\"Mass_and_Pt_IN_H{HOW_MANY_H_TO_TRAINING}_Z{HOW_MANY_Z_TO_TRAINING}_Lambda{lamb}_FLAT500k_genPtCut400_msd5_met50\"\n",
    "            inputParticle = Input(shape=(particlesConsidered, entriesPerParticle), name=\"inputParticle\")\n",
    "\n",
    "            XdotRR = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RR, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRR\")(inputParticle)\n",
    "            XdotRS = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RS, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRS\")(inputParticle)\n",
    "            Bpp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bpp\")([XdotRR, XdotRS])\n",
    "\n",
    "            convOneParticle = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneParticle\")(Bpp)\n",
    "            convTwoParticle = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoParticle\")(convOneParticle)\n",
    "            convThreeParticle = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeParticle\")(convTwoParticle)\n",
    "\n",
    "            Epp = BatchNormalization(momentum=0.6, name=\"Epp\")(convThreeParticle)\n",
    "\n",
    "            # Secondary vertex data interaction NN\n",
    "            inputSV = Input(shape=(svConsidered, entriesPerSV), name=\"inputSV\")\n",
    "\n",
    "            XdotRK = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RK, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRK\")(inputParticle)\n",
    "            YdotRV = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RV, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"YdotRV\")(inputSV)\n",
    "            Bvp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bvp\")([XdotRK, YdotRV])\n",
    "\n",
    "            convOneSV = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneSV\")(Bvp)\n",
    "            convTwoSV = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoSV\")(convOneSV)\n",
    "            convThreeSV = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeSV\")(convTwoSV)\n",
    "\n",
    "            Evp = BatchNormalization(momentum=0.6, name=\"Evp\")(convThreeSV)\n",
    "\n",
    "\n",
    "            # Event Level Info\n",
    "\n",
    "            inputEvent = Input(shape=(eventDataLength, ), name=\"inputEvent\")\n",
    "\n",
    "\n",
    "            # Combined prediction NN\n",
    "            EppBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RRT, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"EppBar\")(Epp)\n",
    "            EvpBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RKT, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"EvpBar\")(Evp)\n",
    "            C = Lambda(lambda listOfTensors: tf.concat((listOfTensors[0], listOfTensors[1], listOfTensors[2]), axis=2), name=\"C\")(\n",
    "                [inputParticle, EppBar, EvpBar])\n",
    "\n",
    "            convPredictOne = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convPredictOne\")(C)\n",
    "            convPredictTwo = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convPredictTwo\")(convPredictOne)\n",
    "\n",
    "            O = Conv1D(24, kernel_size=1, activation=\"relu\", name=\"O\")(convPredictTwo)\n",
    "\n",
    "            # Calculate output\n",
    "            OBar = Lambda(lambda tensor: K.sum(tensor, axis=1), name=\"OBar\")(O)\n",
    "\n",
    "            Concatted = Concatenate()([OBar, inputEvent])\n",
    "\n",
    "            denseEndOne = Dense(50, activation=\"relu\", name=\"denseEndOne\")(Concatted)\n",
    "            normEndOne = BatchNormalization(momentum=0.6, name=\"normEndOne\")(denseEndOne)\n",
    "            denseEndTwo = Dense(20, activation=\"relu\", name=\"denseEndTwo\")(normEndOne)\n",
    "            denseEndThree = Dense(10, activation=\"relu\", name=\"denseEndThree\")(denseEndTwo)\n",
    "            output = Dense(2,name=\"output\")(denseEndThree)\n",
    "\n",
    "            X_train = np.vstack((X_htt[:HOW_MANY_H_TO_TRAINING], X_z[:HOW_MANY_Z_TO_TRAINING], X_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            Xalt_train = np.vstack((Xalt_htt[:HOW_MANY_H_TO_TRAINING], Xalt_z[:HOW_MANY_Z_TO_TRAINING], Xalt_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            Xevt_train = np.vstack((Xevt_htt[:HOW_MANY_H_TO_TRAINING], Xevt_z[:HOW_MANY_Z_TO_TRAINING], Xevt_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            y_train = np.concatenate((y_htt[:HOW_MANY_H_TO_TRAINING,0:2], y_z[:HOW_MANY_Z_TO_TRAINING,0:2], y_flat[:HOW_MANY_FLAT_TO_TRAINING,0:2]))\n",
    "            #plt.hist(y_train[:,0])\n",
    "            #plt.hist(y_train[:,1])\n",
    "            #plt.show()\n",
    "            X_test_htt = X_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            X_test_z   = X_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            Xalt_test_htt = Xalt_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            Xalt_test_z   = Xalt_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            Xevt_test_htt = Xevt_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            Xevt_test_z = Xevt_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            y_test_htt = y_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE,0:2]\n",
    "            y_test_z  = y_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE,0:2]\n",
    "\n",
    "            X_test_flat   = X_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            Xalt_test_flat   = Xalt_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            Xevt_test_flat = Xevt_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            y_test_flat  = y_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE,0:2]\n",
    "\n",
    "            X_train, Xalt_train, Xevt_train, y_train = sklearn.utils.shuffle(X_train, Xalt_train, Xevt_train, y_train)\n",
    "            X_train, X_val, Xalt_train, Xalt_val, Xevt_train, Xevt_val, y_train, y_val = train_test_split(X_train, Xalt_train, Xevt_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "            #print(X_train[0] )\n",
    "            #print(X_test_htt[0])\n",
    "            \n",
    "\n",
    "            model = Model(inputs=[inputParticle, inputSV, inputEvent], outputs=[output])\n",
    "            opt = Adam(lr=0.001)\n",
    "            model.compile(optimizer=opt, loss=mean_sqrt_error, metrics=[mean_sqrt_error], run_eagerly=True)\n",
    "            modelCallbacks = [EarlyStopping(patience=10),ModelCheckpoint(filepath=\"./weights/\"+modelName+\".h5\", save_weights_only=True, save_best_only=True)]\n",
    "            PredictionCallback(model, [X_val, Xalt_val, Xevt_val], y_val )\n",
    "            history = model.fit([X_train, Xalt_train, Xevt_train], y_train, epochs=numberOfEpochs, batch_size=batchSize,\n",
    "                                callbacks=modelCallbacks,\n",
    "                                validation_data=([X_val, Xalt_val, Xevt_val], y_val))\n",
    "\n",
    "\n",
    "            model.save(\"./saved_model/\"+modelName)\n",
    "            model.load_weights(\"./weights/\"+modelName+\".h5\")\n",
    "            predictions_htt = model.predict([X_test_htt, Xalt_test_htt, Xevt_test_htt])\n",
    "            predictions_z = model.predict([X_test_z, Xalt_test_z, Xevt_test_z])\n",
    "            \n",
    "            \n",
    "\n",
    "            #_, bins_z, patches_z = plt.hist(predictions_z[:,0],bins=np.arange(50,200,0.5),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            #_, bins_h, patches_h = plt.hist(predictions_htt[:,0],bins=np.arange(50,200,0.5),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            #plt.hist(predictions_z[:,0],bins=np.arange(50,200,1),linewidth=2,density=True,histtype=\"step\",color=\"C0\",label=\"Z\");\n",
    "            #plt.hist(predictions_htt[:,0],bins=np.arange(50,200,1),linewidth=2,density=True,histtype=\"step\",color=\"C1\",label=\"Higgs\");\n",
    "            #plt.axvline(x=np.quantile(predictions_htt[:,0],0.1),color='C1',linewidth=2)\n",
    "            #for i in range(len(bins_z)-1):\n",
    "            #    #print(bins_h[i])\n",
    "            #    if bins_z[i] > np.quantile(predictions_htt[:,0],0.1):\n",
    "            #        plt.setp(patches_z[i], facecolor=\"C0\",alpha=0.3)\n",
    "\n",
    "\n",
    "            #for i in range(len(bins_h)-1):\n",
    "            #    #print(bins_h[i])\n",
    "            #    if bins_h[i] > np.quantile(predictions_htt[:,0],0.1):\n",
    "            #        plt.setp(patches_h[i], facecolor=\"C1\",alpha=0.3)\n",
    "\n",
    "            #plt.legend(loc='upper left')    \n",
    "            #plt.text(np.quantile(predictions_htt[:,0],0.1),0.015,'90% of Higgs to the Right',ha='center', va='center',rotation='vertical', backgroundcolor='None')\n",
    "            #score_complement=stats.percentileofscore(predictions_z[:,0],np.quantile(predictions_htt[:,0],0.1))/100\n",
    "            #score = 1.-score_complement\n",
    "            #summary = r'$P(m_{Z,reg} > cutoff)=$'f'{score:.3f}''\\n'r'$1 / \\sqrt{P} = $'f'{1/np.sqrt(score):.3f}'\n",
    "            #anchored_text = AnchoredText(summary, loc=1)\n",
    "\n",
    "            #ax = plt.gca()\n",
    "            #ax.add_artist(anchored_text)\n",
    "            #plt.title(f'{HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed, Lambda={lamb}')\n",
    "\n",
    "            #plt.savefig(prefix+f'summaryscore-{1/np.sqrt(score):.4f}'+modelName+f'.png')\n",
    "            #plt.show()\n",
    "\n",
    "            #count_z, bins_z, patches_z = plt.hist(predictions_z[:,0],bins=np.arange(50,210,10),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            #count_h, bins_h, patches_h = plt.hist(predictions_htt[:,0],bins=np.arange(50,210,10),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            #plt.hist(predictions_z[:,0],bins=np.arange(50,210,10),linewidth=2,density=True,histtype=\"step\",color=\"C0\",label=\"Z\");\n",
    "            #plt.hist(predictions_htt[:,0],bins=np.arange(50,210,10),linewidth=2,density=True,histtype=\"step\",color=\"C1\",label=\"Higgs\");\n",
    "            #plt.axvline(x=110,color='r',linewidth=2,alpha=0.4)\n",
    "            #plt.axvline(x=140,color='r',linewidth=2,alpha=0.4)\n",
    "            #S = 0.\n",
    "            #B = 0.\n",
    "            #for i in range(len(bins_z)-1):\n",
    "            #    #print(bins_h[i])\n",
    "            #    if bins_z[i] in [110,120,130]:\n",
    "            #        plt.setp(patches_z[i], facecolor=\"C0\",alpha=0.2)\n",
    "            #        plt.setp(patches_h[i], facecolor=\"C1\",alpha=0.2)\n",
    "            #        S += count_h[i]\n",
    "            #        B += count_z[i]\n",
    "\n",
    "            #significance = S/math.sqrt(B)\n",
    "            #summary = r'$\\frac{P(m_{H,reg} \\in [110,140])}{\\sqrt{P(m_{Z,reg} \\in [110,140])}}=$'f'{significance:.3f}'\n",
    "            #anchored_text = AnchoredText(summary, loc=1,prop=dict(fontsize=15))\n",
    "            #plt.legend(loc='upper left')    \n",
    "\n",
    "            #ax = plt.gca()\n",
    "            #ax.add_artist(anchored_text)\n",
    "            #plt.title(f'{HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed, Lambda={lamb}')\n",
    "\n",
    "            #plt.savefig(prefix+f'significance-{np.sqrt(significance):.4f}'+modelName+f'.png')\n",
    "\n",
    "            #plt.show()\n",
    "            \n",
    "            \n",
    "            #predictions_qcd = model.predict([X_qcd, Xalt_qcd, Xevt_qcd])\n",
    "            #mass_qcd = predictions_qcd[:,0]\n",
    "    \n",
    "            #H_rec = (mass_qcd.flatten()-y_test_htt[:,0])/y_test_htt[:,0]\n",
    "\n",
    "            #plt.hist(mass_qcd,bins=np.arange(0,300,1),color='C0',label='Reconstructed Mass',alpha=0.2,density=True);\n",
    "            #plt.hist(y_qcd[:10000,0],bins=np.arange(0,300,10),color='C1',label='gen Mass',alpha=0.2,density=True)\n",
    "            #plt.axvline(x=125,color='r')\n",
    "            #plt.text(110,500,'m=125',rotation=90,fontsize=15)\n",
    "            #plt.title(f'QCD, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            #plt.legend()\n",
    "            #plt.xlabel('M (GeV)')\n",
    "            #plt.ylabel('Density')\n",
    "\n",
    "            #summary = f'''mean:{np.mean(mass_qcd)}\n",
    "            #std:{np.std(mass_qcd)}'''\n",
    "\n",
    "            #anchored_text = AnchoredText(summary, loc=1)\n",
    "            #ax = plt.gca()\n",
    "            #ax.add_artist(anchored_text)\n",
    "            #plt.savefig(prefix+modelName+f'-QCD-hadhad-mass.png')\n",
    "            #plt.show()\n",
    "\n",
    "            #plt.hist(predictions_qcd[:,1],bins=np.arange(0,1000,10),color='r',label='Reconstructed Pt',alpha=0.2,density=True);\n",
    "            #plt.hist(y_qcd[:10000,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2,density=True)\n",
    "            #plt.title(f'QCD, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            #plt.xlabel('pT (GeV)')\n",
    "            #plt.legend()\n",
    "            #plt.ylabel('Density')\n",
    "            #plt.savefig(prefix+modelName+f'-QCD-hadhad-pT.png')\n",
    "            #plt.show()\n",
    "            \n",
    "            #pt_response = (predictions_htt[:,1]-y_test_htt[:,1])/y_test_htt[:,1]\n",
    "\n",
    "            #plt.hist(pt_response,bins=np.arange(-0.5,0.5,.02),color='r',label=r'$p_{T}$ Response',alpha=0.2, density=True);\n",
    "            #plt.hist(y_htt[:,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2)\n",
    "            #plt.title(f'Htt, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            #plt.xlabel(r'$(p_{T,reg}-p_{T,gen})/p_{T,gen}$')\n",
    "            #plt.ylabel('Density')\n",
    "            #plt.legend(loc='upper left')\n",
    "\n",
    "            #summary = f'''mean: {np.mean(pt_response):.3f}\\nstd:   {np.std(pt_response):.3f}'''\n",
    "\n",
    "            #anchored_text = AnchoredText(summary, loc=1)\n",
    "            #ax = plt.gca()\n",
    "            #ax.add_artist(anchored_text)\n",
    "\n",
    "\n",
    "            #plt.savefig(prefix+modelName+f'-Higgs-hadhad-pT-Response-summary.png')\n",
    "            #plt.show()\n",
    "            \n",
    "            massH = predictions_htt[:,0]\n",
    "            H_rec = (massH.flatten()-y_test_htt[:,0])/y_test_htt[:,0]\n",
    "            \n",
    "            massZ = predictions_z[:,0]\n",
    "            Z_rec = (massZ.flatten()-y_test_z[:,0])/y_test_z[:,0]\n",
    "\n",
    "            predictions_flat = model.predict([X_test_flat, Xalt_test_flat, Xevt_test_flat])\n",
    "            massFlat = predictions_flat[:,0]\n",
    "            Flat_rec = (massFlat.flatten()-y_test_flat[:,0])/y_test_flat[:,0]            \n",
    "            \n",
    "            n, b, patches = plt.hist(H_rec,bins=np.arange(-0.75,0.75,0.01),alpha=.9,histtype='step',density=True,color='r',label='H');\n",
    "            H_mode = b[np.argmax(n)]\n",
    "\n",
    "            n, b, patches = plt.hist(Z_rec,bins=np.arange(-0.75,0.75,0.01),alpha=.9,histtype='step',density=True,color='g',label='Z');\n",
    "            Z_mode = b[np.argmax(n)]\n",
    "            n, b, patches = plt.hist(Flat_rec,bins=np.arange(-0.75,0.75,0.01),alpha=.9,histtype='step',density=True,color='b',label='Flat');\n",
    "            Flat_mode = b[np.argmax(n)]\n",
    "            plt.xlim([-1.2, 1.2])\n",
    "            print(Z_mode)\n",
    "            plt.text(0.75,1.3,f'Flat Mean: {np.mean(Flat_rec):.3f}, Mode:{Flat_mode:.3f}',fontsize=12,transform=ax.transAxes)\n",
    "            plt.text(0.75,1.4,f'H Mean: {np.mean(H_rec):.3f}, Mode:{H_mode:.3f}',fontsize=12,transform=ax.transAxes)\n",
    "            plt.text(0.75,1.5,f'Z Mean: {np.mean(Z_rec):.3f}, Mode:{Z_mode:.3f}',fontsize=12,transform=ax.transAxes)\n",
    "            \n",
    "            \n",
    "\n",
    "            plt.xlabel(r'$(m_{reg}-m_{gen})/m_{gen}$',fontsize=15)\n",
    "            plt.ylabel(r'$density$',fontsize=15)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(prefix+f'Zmode:{Z_mode:.3f}-'+modelName+'-mass-response.png')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            K.clear_session() \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_htt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xalt_test_htt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for HOW_MANY_H_TO_TRAINING, HOW_MANY_Z_TO_TRAINING, lamb in wps:\n",
    "\n",
    "            modelName=f\"MLP\"\n",
    "            #modelName=f\"Mass_and_Pt_IN_H{HOW_MANY_H_TO_TRAINING}_Z{HOW_MANY_Z_TO_TRAINING}_Lambda{lamb}_FLAT500k_genPtCut400_msd5_met50\"\n",
    "            inputParticle = Input(shape=(particlesConsidered, entriesPerParticle), name=\"inputParticle\")\n",
    "\n",
    "            XdotRR = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RR, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRR\")(inputParticle)\n",
    "            XdotRS = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RS, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRS\")(inputParticle)\n",
    "            Bpp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bpp\")([XdotRR, XdotRS])\n",
    "\n",
    "            convOneParticle = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneParticle\")(Bpp)\n",
    "            convTwoParticle = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoParticle\")(convOneParticle)\n",
    "            convThreeParticle = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeParticle\")(convTwoParticle)\n",
    "\n",
    "            Epp = BatchNormalization(momentum=0.6, name=\"Epp\")(convThreeParticle)\n",
    "\n",
    "            # Secondary vertex data interaction NN\n",
    "            inputSV = Input(shape=(svConsidered, entriesPerSV), name=\"inputSV\")\n",
    "\n",
    "            XdotRK = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RK, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRK\")(inputParticle)\n",
    "            YdotRV = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RV, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"YdotRV\")(inputSV)\n",
    "            Bvp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bvp\")([XdotRK, YdotRV])\n",
    "\n",
    "            convOneSV = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneSV\")(Bvp)\n",
    "            convTwoSV = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoSV\")(convOneSV)\n",
    "            convThreeSV = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeSV\")(convTwoSV)\n",
    "\n",
    "            Evp = BatchNormalization(momentum=0.6, name=\"Evp\")(convThreeSV)\n",
    "\n",
    "\n",
    "            # Event Level Info\n",
    "\n",
    "            inputEvent = Input(shape=(eventDataLength, ), name=\"inputEvent\")\n",
    "\n",
    "\n",
    "            # Combined prediction NN\n",
    "            EppBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RRT, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"EppBar\")(Epp)\n",
    "            EvpBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RKT, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"EvpBar\")(Evp)\n",
    "            C = Lambda(lambda listOfTensors: tf.concat((listOfTensors[0], listOfTensors[1], listOfTensors[2]), axis=2), name=\"C\")(\n",
    "                [inputParticle, EppBar, EvpBar])\n",
    "\n",
    "            convPredictOne = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convPredictOne\")(C)\n",
    "            convPredictTwo = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convPredictTwo\")(convPredictOne)\n",
    "\n",
    "            O = Conv1D(24, kernel_size=1, activation=\"relu\", name=\"O\")(convPredictTwo)\n",
    "\n",
    "            # Calculate output\n",
    "            OBar = Lambda(lambda tensor: K.sum(tensor, axis=1), name=\"OBar\")(O)\n",
    "\n",
    "            Concatted = Concatenate()([inputEvent])\n",
    "\n",
    "            denseEndOne = Dense(50, activation=\"relu\", name=\"denseEndOne\")(Concatted)\n",
    "            normEndOne = BatchNormalization(momentum=0.6, name=\"normEndOne\")(denseEndOne)\n",
    "            denseEndTwo = Dense(20, activation=\"relu\", name=\"denseEndTwo\")(normEndOne)\n",
    "            denseEndThree = Dense(10, activation=\"relu\", name=\"denseEndThree\")(denseEndTwo)\n",
    "            output = Dense(2,name=\"output\")(denseEndThree)\n",
    "\n",
    "            X_train = np.vstack((X_htt[:HOW_MANY_H_TO_TRAINING], X_z[:HOW_MANY_Z_TO_TRAINING], X_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            Xalt_train = np.vstack((Xalt_htt[:HOW_MANY_H_TO_TRAINING], Xalt_z[:HOW_MANY_Z_TO_TRAINING], Xalt_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            Xevt_train = np.vstack((Xevt_htt[:HOW_MANY_H_TO_TRAINING], Xevt_z[:HOW_MANY_Z_TO_TRAINING], Xevt_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            y_train = np.concatenate((y_htt[:HOW_MANY_H_TO_TRAINING,0:2], y_z[:HOW_MANY_Z_TO_TRAINING,0:2], y_flat[:HOW_MANY_FLAT_TO_TRAINING,0:2]))\n",
    "            plt.hist(y_train[:,0])\n",
    "            #plt.hist(y_train[:,1])\n",
    "            plt.show()\n",
    "            X_test_htt = X_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            X_test_z   = X_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            Xalt_test_htt = Xalt_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            Xalt_test_z   = Xalt_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            Xevt_test_htt = Xevt_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            Xevt_test_z = Xevt_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            y_test_htt = y_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE,0:2]\n",
    "            y_test_z  = y_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE,0:2]\n",
    "\n",
    "            X_test_flat   = X_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            Xalt_test_flat   = Xalt_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            Xevt_test_flat = Xevt_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            y_test_flat  = y_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE,0:2]\n",
    "\n",
    "            X_train, Xalt_train, Xevt_train, y_train = sklearn.utils.shuffle(X_train, Xalt_train, Xevt_train, y_train)\n",
    "            X_train, X_val, Xalt_train, Xalt_val, Xevt_train, Xevt_val, y_train, y_val = train_test_split(X_train, Xalt_train, Xevt_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "            #print(X_train[0] )\n",
    "            #print(X_test_htt[0])\n",
    "            \n",
    "\n",
    "            model = Model(inputs=[inputParticle, inputSV, inputEvent], outputs=[output])\n",
    "            opt = Adam(lr=0.001)\n",
    "            tf.config.run_functions_eagerly(True)\n",
    "            model.compile(optimizer=opt, loss=mean_sqrt_error, metrics=[mean_sqrt_error], run_eagerly=True)\n",
    "            modelCallbacks = [EarlyStopping(patience=10),ModelCheckpoint(filepath=\"./weights/\"+modelName+\".h5\", save_weights_only=True, save_best_only=True)]\n",
    "            #PredictionCallback(model, [X_val, Xalt_val, Xevt_val], y_val )\n",
    "            history = model.fit([X_train, Xalt_train, Xevt_train], y_train, epochs=numberOfEpochs, batch_size=batchSize,\n",
    "                                callbacks=modelCallbacks,\n",
    "                                validation_data=([X_val, Xalt_val, Xevt_val], y_val))\n",
    "\n",
    "\n",
    "            model.save(\"./saved_model/\"+modelName)\n",
    "            model.load_weights(\"./weights/\"+modelName+\".h5\")\n",
    "            predictions_htt = model.predict([X_test_htt, Xalt_test_htt, Xevt_test_htt])\n",
    "            predictions_z = model.predict([X_test_z, Xalt_test_z, Xevt_test_z])\n",
    "            \n",
    "            \n",
    "\n",
    "            _, bins_z, patches_z = plt.hist(predictions_z[:,0],bins=np.arange(50,200,0.5),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            _, bins_h, patches_h = plt.hist(predictions_htt[:,0],bins=np.arange(50,200,0.5),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            plt.hist(predictions_z[:,0],bins=np.arange(50,200,1),linewidth=2,density=True,histtype=\"step\",color=\"C0\",label=\"Z\");\n",
    "            plt.hist(predictions_htt[:,0],bins=np.arange(50,200,1),linewidth=2,density=True,histtype=\"step\",color=\"C1\",label=\"Higgs\");\n",
    "            plt.axvline(x=np.quantile(predictions_htt[:,0],0.1),color='C1',linewidth=2)\n",
    "            for i in range(len(bins_z)-1):\n",
    "                #print(bins_h[i])\n",
    "                if bins_z[i] > np.quantile(predictions_htt[:,0],0.1):\n",
    "                    plt.setp(patches_z[i], facecolor=\"C0\",alpha=0.3)\n",
    "\n",
    "\n",
    "            for i in range(len(bins_h)-1):\n",
    "                #print(bins_h[i])\n",
    "                if bins_h[i] > np.quantile(predictions_htt[:,0],0.1):\n",
    "                    plt.setp(patches_h[i], facecolor=\"C1\",alpha=0.3)\n",
    "\n",
    "            plt.legend(loc='upper left')    \n",
    "            plt.text(np.quantile(predictions_htt[:,0],0.1),0.015,'90% of Higgs to the Right',ha='center', va='center',rotation='vertical', backgroundcolor='None')\n",
    "            score_complement=stats.percentileofscore(predictions_z[:,0],np.quantile(predictions_htt[:,0],0.1))/100\n",
    "            score = 1.-score_complement\n",
    "            summary = r'$P(m_{Z,reg} > cutoff)=$'f'{score:.3f}''\\n'r'$1 / \\sqrt{P} = $'f'{1/np.sqrt(score):.3f}'\n",
    "            anchored_text = AnchoredText(summary, loc=1)\n",
    "\n",
    "            ax = plt.gca()\n",
    "            ax.add_artist(anchored_text)\n",
    "            plt.title(f'{HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed, Lambda={lamb}')\n",
    "\n",
    "            plt.savefig(prefix+f'summaryscore-{1/np.sqrt(score):.4f}'+modelName+f'.png')\n",
    "            plt.show()\n",
    "\n",
    "            count_z, bins_z, patches_z = plt.hist(predictions_z[:,0],bins=np.arange(50,210,10),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            count_h, bins_h, patches_h = plt.hist(predictions_htt[:,0],bins=np.arange(50,210,10),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            plt.hist(predictions_z[:,0],bins=np.arange(50,210,10),linewidth=2,density=True,histtype=\"step\",color=\"C0\",label=\"Z\");\n",
    "            plt.hist(predictions_htt[:,0],bins=np.arange(50,210,10),linewidth=2,density=True,histtype=\"step\",color=\"C1\",label=\"Higgs\");\n",
    "            #plt.axvline(x=110,color='r',linewidth=2,alpha=0.4)\n",
    "            #plt.axvline(x=140,color='r',linewidth=2,alpha=0.4)\n",
    "            S = 0.\n",
    "            B = 0.\n",
    "            for i in range(len(bins_z)-1):\n",
    "                #print(bins_h[i])\n",
    "                if bins_z[i] in [110,120,130]:\n",
    "                    plt.setp(patches_z[i], facecolor=\"C0\",alpha=0.2)\n",
    "                    plt.setp(patches_h[i], facecolor=\"C1\",alpha=0.2)\n",
    "                    S += count_h[i]\n",
    "                    B += count_z[i]\n",
    "\n",
    "            significance = S/math.sqrt(B)\n",
    "            summary = r'$\\frac{P(m_{H,reg} \\in [110,140])}{\\sqrt{P(m_{Z,reg} \\in [110,140])}}=$'f'{significance:.3f}'\n",
    "            anchored_text = AnchoredText(summary, loc=1,prop=dict(fontsize=15))\n",
    "            plt.legend(loc='upper left')    \n",
    "\n",
    "            ax = plt.gca()\n",
    "            ax.add_artist(anchored_text)\n",
    "            plt.title(f'{HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed, Lambda={lamb}')\n",
    "\n",
    "            plt.savefig(prefix+f'significance-{np.sqrt(significance):.4f}'+modelName+f'.png')\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            predictions_qcd = model.predict([X_qcd, Xalt_qcd, Xevt_qcd])\n",
    "            mass_qcd = predictions_qcd[:,0]\n",
    "    \n",
    "            #H_rec = (mass_qcd.flatten()-y_test_htt[:,0])/y_test_htt[:,0]\n",
    "\n",
    "            plt.hist(mass_qcd,bins=np.arange(0,300,1),color='C0',label='Reconstructed Mass',alpha=0.2,density=True);\n",
    "            plt.hist(y_qcd[:10000,0],bins=np.arange(0,300,10),color='C1',label='gen Mass',alpha=0.2,density=True)\n",
    "            plt.axvline(x=125,color='r')\n",
    "            #plt.text(110,500,'m=125',rotation=90,fontsize=15)\n",
    "            plt.title(f'QCD, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            plt.legend()\n",
    "            plt.xlabel('M (GeV)')\n",
    "            plt.ylabel('Density')\n",
    "\n",
    "            #summary = f'''mean:{np.mean(mass_qcd)}\n",
    "            #std:{np.std(mass_qcd)}'''\n",
    "\n",
    "            #anchored_text = AnchoredText(summary, loc=1)\n",
    "            #ax = plt.gca()\n",
    "            #ax.add_artist(anchored_text)\n",
    "            plt.savefig(prefix+modelName+f'-QCD-hadhad-mass.png')\n",
    "            plt.show()\n",
    "\n",
    "            plt.hist(predictions_qcd[:,1],bins=np.arange(0,1000,10),color='r',label='Reconstructed Pt',alpha=0.2,density=True);\n",
    "            plt.hist(y_qcd[:10000,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2,density=True)\n",
    "            plt.title(f'QCD, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            plt.xlabel('pT (GeV)')\n",
    "            plt.legend()\n",
    "            plt.ylabel('Density')\n",
    "            plt.savefig(prefix+modelName+f'-QCD-hadhad-pT.png')\n",
    "            plt.show()\n",
    "            \n",
    "            pt_response = (predictions_htt[:,1]-y_test_htt[:,1])/y_test_htt[:,1]\n",
    "\n",
    "            plt.hist(pt_response,bins=np.arange(-0.5,0.5,.02),color='r',label=r'$p_{T}$ Response',alpha=0.2, density=True);\n",
    "            #plt.hist(y_htt[:,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2)\n",
    "            plt.title(f'Htt, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            plt.xlabel(r'$(p_{T,reg}-p_{T,gen})/p_{T,gen}$')\n",
    "            plt.ylabel('Density')\n",
    "            plt.legend(loc='upper left')\n",
    "\n",
    "            summary = f'''mean: {np.mean(pt_response):.3f}\\nstd:   {np.std(pt_response):.3f}'''\n",
    "\n",
    "            anchored_text = AnchoredText(summary, loc=1)\n",
    "            ax = plt.gca()\n",
    "            ax.add_artist(anchored_text)\n",
    "\n",
    "\n",
    "            plt.savefig(prefix+modelName+f'-Higgs-hadhad-pT-Response-summary.png')\n",
    "            plt.show()\n",
    "            \n",
    "            massH = predictions_htt[:,0]\n",
    "            H_rec = (massH.flatten()-y_test_htt[:,0])/y_test_htt[:,0]\n",
    "            \n",
    "            massZ = predictions_z[:,0]\n",
    "            Z_rec = (massZ.flatten()-y_test_z[:,0])/y_test_z[:,0]\n",
    "\n",
    "            predictions_flat = model.predict([X_test_flat, Xalt_test_flat, Xevt_test_flat])\n",
    "            massFlat = predictions_flat[:,0]\n",
    "            Flat_rec = (massFlat.flatten()-y_test_flat[:,0])/y_test_flat[:,0]            \n",
    "            \n",
    "            plt.hist(H_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='r',label='H');\n",
    "            plt.hist(Z_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='g',label='Z');\n",
    "            plt.hist(Flat_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='b',label='Flat');\n",
    "            plt.xlabel(r'$(m_{reg}-m_{gen})/m_{gen}$',fontsize=15)\n",
    "            plt.ylabel(r'$density$',fontsize=15)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(prefix+modelName+'-mass-response.png')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            #K.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/nobackup/users/sangeon/phitautau_figures/UL/Apr2022/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for HOW_MANY_H_TO_TRAINING, HOW_MANY_Z_TO_TRAINING, lamb in wps:\n",
    "\n",
    "            modelName=f\"NEW_UL_NoFeatSel_H{HOW_MANY_H_TO_TRAINING}_Z{HOW_MANY_Z_TO_TRAINING}_FLAT{HOW_MANY_FLAT_TO_TRAINING}_Lambda{lamb}_hadhad\"\n",
    "            #modelName=f\"Mass_and_Pt_IN_H{HOW_MANY_H_TO_TRAINING}_Z{HOW_MANY_Z_TO_TRAINING}_Lambda{lamb}_FLAT500k_genPtCut400_msd5_met50\"\n",
    "            inputParticle = Input(shape=(particlesConsidered, entriesPerParticle), name=\"inputParticle\")\n",
    "\n",
    "            XdotRR = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RR, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRR\")(inputParticle)\n",
    "            XdotRS = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RS, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRS\")(inputParticle)\n",
    "            Bpp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bpp\")([XdotRR, XdotRS])\n",
    "\n",
    "            convOneParticle = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneParticle\")(Bpp)\n",
    "            convTwoParticle = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoParticle\")(convOneParticle)\n",
    "            convThreeParticle = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeParticle\")(convTwoParticle)\n",
    "\n",
    "            Epp = BatchNormalization(momentum=0.6, name=\"Epp\")(convThreeParticle)\n",
    "\n",
    "            # Secondary vertex data interaction NN\n",
    "            inputSV = Input(shape=(svConsidered, entriesPerSV), name=\"inputSV\")\n",
    "\n",
    "            XdotRK = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RK, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"XdotRK\")(inputParticle)\n",
    "            YdotRV = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RV, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"YdotRV\")(inputSV)\n",
    "            Bvp = Lambda(lambda tensorList: tf.concat((tensorList[0], tensorList[1]), axis=2), name=\"Bvp\")([XdotRK, YdotRV])\n",
    "\n",
    "            convOneSV = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convOneSV\")(Bvp)\n",
    "            convTwoSV = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convTwoSV\")(convOneSV)\n",
    "            convThreeSV = Conv1D(20, kernel_size=1, activation=\"relu\", name=\"convThreeSV\")(convTwoSV)\n",
    "\n",
    "            Evp = BatchNormalization(momentum=0.6, name=\"Evp\")(convThreeSV)\n",
    "\n",
    "\n",
    "            # Event Level Info\n",
    "\n",
    "            inputEvent = Input(shape=(eventDataLength, ), name=\"inputEvent\")\n",
    "\n",
    "\n",
    "            # Combined prediction NN\n",
    "            EppBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RRT, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"EppBar\")(Epp)\n",
    "            EvpBar = Lambda(lambda tensor: tf.transpose(tf.tensordot(tf.transpose(tensor, perm=(0, 2, 1)), RKT, axes=[[2], [0]]),\n",
    "                                                        perm=(0, 2, 1)), name=\"EvpBar\")(Evp)\n",
    "            C = Lambda(lambda listOfTensors: tf.concat((listOfTensors[0], listOfTensors[1], listOfTensors[2]), axis=2), name=\"C\")(\n",
    "                [inputParticle, EppBar, EvpBar])\n",
    "\n",
    "            convPredictOne = Conv1D(60, kernel_size=1, activation=\"relu\", name=\"convPredictOne\")(C)\n",
    "            convPredictTwo = Conv1D(30, kernel_size=1, activation=\"relu\", name=\"convPredictTwo\")(convPredictOne)\n",
    "\n",
    "            O = Conv1D(24, kernel_size=1, activation=\"relu\", name=\"O\")(convPredictTwo)\n",
    "\n",
    "            # Calculate output\n",
    "            OBar = Lambda(lambda tensor: K.sum(tensor, axis=1), name=\"OBar\")(O)\n",
    "\n",
    "            Concatted = Concatenate()([OBar, inputEvent])\n",
    "\n",
    "            denseEndOne = Dense(50, activation=\"relu\", name=\"denseEndOne\")(Concatted)\n",
    "            normEndOne = BatchNormalization(momentum=0.6, name=\"normEndOne\")(denseEndOne)\n",
    "            denseEndTwo = Dense(20, activation=\"relu\", name=\"denseEndTwo\")(normEndOne)\n",
    "            denseEndThree = Dense(10, activation=\"relu\", name=\"denseEndThree\")(denseEndTwo)\n",
    "            output = Dense(2,name=\"output\")(denseEndThree)\n",
    "\n",
    "            X_train = np.vstack((X_htt[:HOW_MANY_H_TO_TRAINING], X_z[:HOW_MANY_Z_TO_TRAINING], X_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            Xalt_train = np.vstack((Xalt_htt[:HOW_MANY_H_TO_TRAINING], Xalt_z[:HOW_MANY_Z_TO_TRAINING], Xalt_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            Xevt_train = np.vstack((Xevt_htt[:HOW_MANY_H_TO_TRAINING], Xevt_z[:HOW_MANY_Z_TO_TRAINING], Xevt_flat[:HOW_MANY_FLAT_TO_TRAINING]))\n",
    "            y_train = np.concatenate((y_htt[:HOW_MANY_H_TO_TRAINING,0:2], y_z[:HOW_MANY_Z_TO_TRAINING,0:2], y_flat[:HOW_MANY_FLAT_TO_TRAINING,0:2]))\n",
    "            plt.hist(y_train[:,0])\n",
    "            #plt.hist(y_train[:,1])\n",
    "            plt.show()\n",
    "            X_test_htt = X_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            X_test_z   = X_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            Xalt_test_htt = Xalt_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            Xalt_test_z   = Xalt_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            Xevt_test_htt = Xevt_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE]\n",
    "            Xevt_test_z = Xevt_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE]\n",
    "\n",
    "            y_test_htt = y_htt[HOW_MANY_H_TO_TRAINING :HOW_MANY_H_TO_TRAINING +TEST_SIZE,0:2]\n",
    "            y_test_z  = y_z[HOW_MANY_Z_TO_TRAINING :HOW_MANY_Z_TO_TRAINING +TEST_SIZE,0:2]\n",
    "\n",
    "            X_test_flat   = X_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            Xalt_test_flat   = Xalt_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            Xevt_test_flat = Xevt_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE]\n",
    "            y_test_flat  = y_flat[HOW_MANY_FLAT_TO_TRAINING :HOW_MANY_FLAT_TO_TRAINING +TEST_SIZE,0:2]\n",
    "\n",
    "            X_train, Xalt_train, Xevt_train, y_train = sklearn.utils.shuffle(X_train, Xalt_train, Xevt_train, y_train)\n",
    "            X_train, X_val, Xalt_train, Xalt_val, Xevt_train, Xevt_val, y_train, y_val = train_test_split(X_train, Xalt_train, Xevt_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "            #print(X_train[0] )\n",
    "            #print(X_test_htt[0])\n",
    "            \n",
    "\n",
    "            model = Model(inputs=[inputParticle, inputSV, inputEvent], outputs=[output])\n",
    "            opt = Adam(lr=0.001)\n",
    "            model.compile(optimizer=opt, loss=mean_sqrt_error, metrics=[mean_sqrt_error], run_eagerly=True)\n",
    "            modelCallbacks = [EarlyStopping(patience=10),ModelCheckpoint(filepath=\"./weights/\"+modelName+\".h5\", save_weights_only=True, save_best_only=True)]\n",
    "            PredictionCallback(model, [X_val, Xalt_val, Xevt_val], y_val )\n",
    "            #history = model.fit([X_train, Xalt_train, Xevt_train], y_train, epochs=numberOfEpochs, batch_size=batchSize,\n",
    "            #                    callbacks=modelCallbacks,\n",
    "            #                    validation_data=([X_val, Xalt_val, Xevt_val], y_val))\n",
    "\n",
    "\n",
    "            #model.save(\"./saved_model/\"+modelName)\n",
    "            model.load_weights(\"./weights/\"+modelName+\".h5\")\n",
    "            model.save(\"./saved_model/\"+modelName)\n",
    "            predictions_htt = model.predict([X_test_htt, Xalt_test_htt, Xevt_test_htt])\n",
    "            predictions_z = model.predict([X_test_z, Xalt_test_z, Xevt_test_z])\n",
    "            \n",
    "            \n",
    "\n",
    "            _, bins_z, patches_z = plt.hist(predictions_z[:,0],bins=np.arange(50,200,0.5),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            _, bins_h, patches_h = plt.hist(predictions_htt[:,0],bins=np.arange(50,200,0.5),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            plt.hist(predictions_z[:,0],bins=np.arange(50,200,1),linewidth=2,density=True,histtype=\"step\",color=\"C0\",label=\"Z\");\n",
    "            plt.hist(predictions_htt[:,0],bins=np.arange(50,200,1),linewidth=2,density=True,histtype=\"step\",color=\"C1\",label=\"Higgs\");\n",
    "            plt.axvline(x=np.quantile(predictions_htt[:,0],0.1),color='C1',linewidth=2)\n",
    "            for i in range(len(bins_z)-1):\n",
    "                #print(bins_h[i])\n",
    "                if bins_z[i] > np.quantile(predictions_htt[:,0],0.1):\n",
    "                    plt.setp(patches_z[i], facecolor=\"C0\",alpha=0.3)\n",
    "\n",
    "\n",
    "            for i in range(len(bins_h)-1):\n",
    "                #print(bins_h[i])\n",
    "                if bins_h[i] > np.quantile(predictions_htt[:,0],0.1):\n",
    "                    plt.setp(patches_h[i], facecolor=\"C1\",alpha=0.3)\n",
    "\n",
    "            plt.legend(loc='upper left')    \n",
    "            plt.text(np.quantile(predictions_htt[:,0],0.1),0.015,'90% of Higgs to the Right',ha='center', va='center',rotation='vertical', backgroundcolor='None')\n",
    "            score_complement=stats.percentileofscore(predictions_z[:,0],np.quantile(predictions_htt[:,0],0.1))/100\n",
    "            score = 1.-score_complement\n",
    "            summary = r'$P(m_{Z,reg} > cutoff)=$'f'{score:.3f}''\\n'r'$1 / \\sqrt{P} = $'f'{1/np.sqrt(score):.3f}'\n",
    "            anchored_text = AnchoredText(summary, loc=1)\n",
    "\n",
    "            ax = plt.gca()\n",
    "            ax.add_artist(anchored_text)\n",
    "            plt.title(f'{HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed, Lambda={lamb}')\n",
    "\n",
    "            plt.savefig(prefix+f'summaryscore-{1/np.sqrt(score):.4f}'+modelName+f'.png')\n",
    "            plt.show()\n",
    "\n",
    "            count_z, bins_z, patches_z = plt.hist(predictions_z[:,0],bins=np.arange(50,210,10),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            count_h, bins_h, patches_h = plt.hist(predictions_htt[:,0],bins=np.arange(50,210,10),linewidth=1.5,density=True,facecolor=\"None\");\n",
    "            plt.hist(predictions_z[:,0],bins=np.arange(50,210,10),linewidth=2,density=True,histtype=\"step\",color=\"C0\",label=\"Z\");\n",
    "            plt.hist(predictions_htt[:,0],bins=np.arange(50,210,10),linewidth=2,density=True,histtype=\"step\",color=\"C1\",label=\"Higgs\");\n",
    "            #plt.axvline(x=110,color='r',linewidth=2,alpha=0.4)\n",
    "            #plt.axvline(x=140,color='r',linewidth=2,alpha=0.4)\n",
    "            S = 0.\n",
    "            B = 0.\n",
    "            for i in range(len(bins_z)-1):\n",
    "                #print(bins_h[i])\n",
    "                if bins_z[i] in [110,120,130]:\n",
    "                    plt.setp(patches_z[i], facecolor=\"C0\",alpha=0.2)\n",
    "                    plt.setp(patches_h[i], facecolor=\"C1\",alpha=0.2)\n",
    "                    S += count_h[i]\n",
    "                    B += count_z[i]\n",
    "\n",
    "            significance = S/math.sqrt(B)\n",
    "            summary = r'$\\frac{P(m_{H,reg} \\in [110,140])}{\\sqrt{P(m_{Z,reg} \\in [110,140])}}=$'f'{significance:.3f}'\n",
    "            anchored_text = AnchoredText(summary, loc=1,prop=dict(fontsize=15))\n",
    "            plt.legend(loc='upper left')    \n",
    "\n",
    "            ax = plt.gca()\n",
    "            ax.add_artist(anchored_text)\n",
    "            plt.title(f'{HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed, Lambda={lamb}')\n",
    "\n",
    "            plt.savefig(prefix+f'significance-{np.sqrt(significance):.4f}'+modelName+f'.png')\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            predictions_qcd = model.predict([X_qcd, Xalt_qcd, Xevt_qcd])\n",
    "            mass_qcd = predictions_qcd[:,0]\n",
    "    \n",
    "            #H_rec = (mass_qcd.flatten()-y_test_htt[:,0])/y_test_htt[:,0]\n",
    "\n",
    "            plt.hist(mass_qcd,bins=np.arange(0,300,1),color='C0',label='Reconstructed Mass',alpha=0.2,density=True);\n",
    "            plt.hist(y_qcd[:10000,0],bins=np.arange(0,300,10),color='C1',label='gen Mass',alpha=0.2,density=True)\n",
    "            plt.axvline(x=125,color='r')\n",
    "            #plt.text(110,500,'m=125',rotation=90,fontsize=15)\n",
    "            plt.title(f'QCD, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            plt.legend()\n",
    "            plt.xlabel('M (GeV)')\n",
    "            plt.ylabel('Density')\n",
    "\n",
    "            #summary = f'''mean:{np.mean(mass_qcd)}\n",
    "            #std:{np.std(mass_qcd)}'''\n",
    "\n",
    "            #anchored_text = AnchoredText(summary, loc=1)\n",
    "            #ax = plt.gca()\n",
    "            #ax.add_artist(anchored_text)\n",
    "            plt.savefig(prefix+modelName+f'-QCD-hadhad-mass.png')\n",
    "            plt.show()\n",
    "\n",
    "            plt.hist(predictions_qcd[:,1],bins=np.arange(0,1000,10),color='r',label='Reconstructed Pt',alpha=0.2,density=True);\n",
    "            plt.hist(y_qcd[:10000,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2,density=True)\n",
    "            plt.title(f'QCD, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            plt.xlabel('pT (GeV)')\n",
    "            plt.legend()\n",
    "            plt.ylabel('Density')\n",
    "            plt.savefig(prefix+modelName+f'-QCD-hadhad-pT.png')\n",
    "            plt.show()\n",
    "            \n",
    "            pt_response = (predictions_htt[:,1]-y_test_htt[:,1])/y_test_htt[:,1]\n",
    "\n",
    "            plt.hist(pt_response,bins=np.arange(-0.5,0.5,.02),color='r',label=r'$p_{T}$ Response',alpha=0.2, density=True);\n",
    "            #plt.hist(y_htt[:,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2)\n",
    "            plt.title(f'Htt, {HOW_MANY_H_TO_TRAINING}H and {HOW_MANY_Z_TO_TRAINING}Z mixed')\n",
    "            plt.xlabel(r'$(p_{T,reg}-p_{T,gen})/p_{T,gen}$')\n",
    "            plt.ylabel('Density')\n",
    "            plt.legend(loc='upper left')\n",
    "\n",
    "            summary = f'''mean: {np.mean(pt_response):.3f}\\nstd:   {np.std(pt_response):.3f}'''\n",
    "\n",
    "            anchored_text = AnchoredText(summary, loc=1)\n",
    "            ax = plt.gca()\n",
    "            ax.add_artist(anchored_text)\n",
    "\n",
    "\n",
    "            plt.savefig(prefix+modelName+f'-Higgs-hadhad-pT-Response-summary.png')\n",
    "            plt.show()\n",
    "            \n",
    "            massH = predictions_htt[:,0]\n",
    "            H_rec = (massH.flatten()-y_test_htt[:,0])/y_test_htt[:,0]\n",
    "            \n",
    "            massZ = predictions_z[:,0]\n",
    "            Z_rec = (massZ.flatten()-y_test_z[:,0])/y_test_z[:,0]\n",
    "\n",
    "            predictions_flat = model.predict([X_test_flat, Xalt_test_flat, Xevt_test_flat])\n",
    "            massFlat = predictions_flat[:,0]\n",
    "            Flat_rec = (massFlat.flatten()-y_test_flat[:,0])/y_test_flat[:,0]            \n",
    "            \n",
    "            plt.hist(H_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='r',label='H');\n",
    "            plt.hist(Z_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='g',label='Z');\n",
    "            plt.hist(Flat_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='b',label='Flat');\n",
    "            plt.xlabel(r'$(m_{reg}-m_{gen})/m_{gen}$',fontsize=15)\n",
    "            plt.ylabel(r'$density$',fontsize=15)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(prefix+modelName+'-mass-response.png')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            #K.clear_session() \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Z_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(H_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_feats = [\n",
    "            \"PF_pt\",\"PF_eta\",\"PF_phi\",\"PF_q\",\"PF_dz\",\"PF_dzerr\",\"PF_dxy\",\"PF_dxyerr\",\"PF_pup\",\"PF_pupnolep\",\"PF_id\",\"PF_trk\",\"PF_vtx\"\n",
    "            ]\n",
    "\n",
    "sv_feats = [\n",
    "            \"sv_dlen\", \"sv_dlenSig\", \"sv_dxy\", \"sv_dxySig\", \"sv_chi2\", \"sv_pAngle\", \"sv_x\", \"sv_y\", \"sv_z\", \"sv_pt\", \"sv_mass\", \"sv_eta\", \"sv_phi\"\n",
    "            ]\n",
    "evt_feats = [\"MET_covXX\",\"MET_covXY\",\"MET_covYY\",\"MET_phi\",\"MET_pt\",\"MET_significance\",\"PuppiMET_pt\",\"PuppiMET_phi\",\"fj_msd\",\"fj_pt\",\"fj_eta\",\"fj_phi\"]\n",
    "target = [\"fj_genMass\",\"fj_genPt\",\"fj_genEta\",\"fj_genPhi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(particle_feats), len(sv_feats), len(evt_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_htt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xalt_test_htt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xevt_test_htt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_htt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_good_and_bad(ax, selected_good, selected_bad, index, whichparticle, whichfeat, binning):\n",
    "    \n",
    "    ax[index].hist(selected_good,label='|res|<0.2', alpha=0.4, density=True, bins=binning)\n",
    "    ax[index].hist(selected_bad,label='res<-0.4', alpha=0.4, density=True, bins=binning)\n",
    "    ax[index].set_xlabel(whichparticle+'_'+whichfeat)\n",
    "    #print(index, selected_good[:10], [binning[0],binning[-1]])\n",
    "    ax[index].set_xlim([binning[0],binning[-1]])\n",
    "    ax[index].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/nobackup/users/sangeon/phitautau_figures/UL/debugging'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (44,8)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig, axs = plt.subplots(2, 12)\n",
    "fig.subplots_adjust(wspace=0.2, \n",
    "                    hspace=0.35)\n",
    "#axs = axs.ravel()\n",
    "Hbad = np.where(H_rec<-0.4)\n",
    "Hgood = np.where((H_rec<0.2)&(H_rec>-0.2)) \n",
    "Zbad = np.where(Z_rec<-0.4)\n",
    "Zgood = np.where((Z_rec<0.2)&(Z_rec>-0.2)) \n",
    "\n",
    "evth_good = Xevt_test_htt[Hgood]\n",
    "evth_bad = Xevt_test_htt[Hbad]\n",
    "\n",
    "evtz_good = Xevt_test_z[Zgood]\n",
    "evtz_bad = Xevt_test_z[Zbad]\n",
    "\n",
    "print(evth_good.shape)\n",
    "print(evth_bad.shape)\n",
    "\n",
    "for i in range(12):\n",
    "    if i<3:\n",
    "        binning = np.linspace(0, 3000,21) \n",
    "    elif i==3:\n",
    "        binning = np.linspace(-4, 4, 21)\n",
    "    elif i==4:\n",
    "        binning = np.linspace(0, 500, 21)\n",
    "    elif i==5:\n",
    "        binning = np.linspace(0, 200, 21)\n",
    "    elif i==7:\n",
    "        binning = np.linspace(-4, 4, 21)\n",
    "    elif i==8:\n",
    "        binning = np.linspace(0, 200, 21)\n",
    "    elif i==9:\n",
    "        binning = np.linspace(100, 1000, 21)\n",
    "    elif i==10:\n",
    "        binning = np.linspace(-4, 4, 21)\n",
    "    elif i==11:\n",
    "        binning = np.linspace(-4, 4, 21)     \n",
    "    else:\n",
    "        binning = np.linspace(0, 500,21) \n",
    "        \n",
    "    #print(bins)\n",
    "    plot_good_and_bad(axs, evth_good[:,i],  evth_bad[:,i], (0,i),'H',evt_feats[i], binning)\n",
    "    plot_good_and_bad(axs, evtz_good[:,i],  evtz_bad[:,i], (1,i),'Z',evt_feats[i], binning)\n",
    "fig = plt.gcf()\n",
    "st = fig.suptitle(\"Evt Features\", fontsize=\"x-large\")\n",
    "st.set_y(0.88)\n",
    "fig.subplots_adjust(top=0.8)\n",
    "\n",
    "fig.savefig(os.path.join(prefix, 'evtfeats.png'))\n",
    "fig.savefig(os.path.join(prefix, 'evtfeats.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(evth_good[:,3],bins=np.linspace(-4, 4, 21) )\n",
    "plt.hist(evth_bad[:,3],bins=np.linspace(-4, 4, 21) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_good_and_bad(ax, selected_good, selected_bad, index, whichparticle, whichfeat):\n",
    "    ax[index].hist(selected_good,label='|res|<0.1')\n",
    "    ax[index].hist(selected_bad,label='res<-0.4')\n",
    "    ax[index].set_xlabel(whichparticle+'_'+whichfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(H_rec<-0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((H_rec<0.1)&(H_rec>-0.1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat,Xalt_flat,Xevt_flat,y_flat,feat_flat = load_flat('/nobackup/users/sangeon/FlatTauTau_user.z',maxevts=0);\n",
    "X_htt,Xalt_htt,Xevt_htt,y_htt,feat_htt = load_flat('/nobackup/users/sangeon/GluGluHToTauTau_user.z');\n",
    "X_z,Xalt_z, Xevt_z, y_z, feat_z = load_flat('/nobackup/users/sangeon/DYJetsToLL.z');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_msd_cut = np.where((Xevt_flat[:,-4]>20)&(Xevt_flat[:,-3]>200)&(y_flat[:,1]>400))\n",
    "htt_msd_cut = np.where((Xevt_htt[:,-4]>20)&(Xevt_htt[:,-3]>200)&(y_htt[:,1]>400))\n",
    "z_msd_cut = np.where((Xevt_z[:,-4]>20)&(Xevt_z[:,-3]>200)&(y_z[:,1]>400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = range(400,900,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"dict_mean_std.pkl\",\"wb\")\n",
    "pickle.dump(dict_mean_std,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test_htt, Xalt_test_htt, Xevt_test_htt])\n",
    "massH = predictions[:,0]\n",
    "H_rec = (massH.flatten()-y_test_htt[:,0])/y_test_htt[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(massH,bins=np.arange(0,300,1));\n",
    "plt.axvline(x=125,color='r')\n",
    "plt.text(110,500,'m=125',rotation=90,fontsize=15)\n",
    "plt.title('Htt, 100k H Z mix')\n",
    "plt.xlabel('M (GeV)')\n",
    "\n",
    "plt.text(160, 600, f'mean: {np.mean(massH):.2f}', fontsize=15)\n",
    "plt.text(160, 500, f'std: {np.std(massH):.2f}', fontsize=15)\n",
    "plt.savefig(prefix+modelName+'-Higgs-hadhad-100k-mass.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions[:,1],bins=np.arange(0,1000,10),color='r',label='Reconstructed Pt',alpha=0.2);\n",
    "plt.hist(y_test_htt[:,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2)\n",
    "plt.title('Htt, 100k H Z mix')\n",
    "plt.xlabel('Pt (GeV)')\n",
    "plt.legend()\n",
    "plt.savefig(prefix+modelName+'-Higgs-hadhad-100k-Pt.pdf')\n",
    "\n",
    "#plt.axvline(x=125,color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test_z, Xalt_test_z, Xevt_test_z])\n",
    "massZ = predictions[:,0]\n",
    "Z_rec = (massZ.flatten()-y_test_z[:,0])/y_test_z[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(massZ,bins=np.arange(0,300,4));\n",
    "plt.axvline(x=91.2,color='r')\n",
    "plt.text(60,2000,'m=91.2',rotation=90,fontsize=15)\n",
    "plt.title('Z, 100k H Z mix')\n",
    "plt.xlabel('M (GeV)')\n",
    "\n",
    "plt.text(160, 2000, f'mean: {np.mean(massZ):.2f}', fontsize=15)\n",
    "plt.text(160, 1400, f'std: {np.std(massZ):.2f}', fontsize=15)\n",
    "#plt.savefig(prefix+modelName+'-Z-hadhad-50000.pdf')\n",
    "plt.savefig(prefix+modelName+'-Z-hadhad-100k-mass.pdf')\n",
    "\n",
    "#plt.text(200, 600, f'mixratio: {mixratio}%', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions[:,1],bins=np.arange(0,1000,10),color='r',label='Reconstructed Pt',alpha=0.2);\n",
    "plt.hist(y_test_z[:,1],bins=np.arange(0,1000,10),color='b',label='gen Pt',alpha=0.2)\n",
    "plt.title('Z, 100k H Z mix')\n",
    "plt.xlabel('Pt (GeV)')\n",
    "plt.legend()\n",
    "plt.savefig(prefix+modelName+'-Z-hadhad-100k-Pt.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test_flat, Xalt_test_flat, Xevt_test_flat])\n",
    "\n",
    "massFlat = predictions[:,0]\n",
    "\n",
    "\n",
    "Flat_rec = (massFlat.flatten()-y_test_flat[:,0])/y_test_flat[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(H_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='r',label='H');\n",
    "plt.hist(Z_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='g',label='Z');\n",
    "plt.hist(Flat_rec,bins=np.arange(-0.75,0.75,0.02),alpha=.9,histtype='step',density=True,color='b',label='Flat');\n",
    "plt.xlabel(r'$(m_{reg}-m_{gen})/m_{gen}$',fontsize=15)\n",
    "plt.ylabel(r'$density$',fontsize=15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(prefix+modelName+'-100k-response.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mass),np.std(mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_z,X_test_z,Xalt_train_z,Xalt_test_z,Xevt_train_z,Xevt_test_z,Y_train_z,Y_test_z,feat_train_z,feat_test_z = load('/nobackup/users/sangeon/DYJetsToLL_0.z');\n",
    "\n",
    "y_train_z = Y_train_z[:,0]\n",
    "\n",
    "X_train_z = (X_train_z-part_mean)/part_std\n",
    "Xalt_train_z = (Xalt_train_z-alt_mean)/alt_std\n",
    "Xevt_train_z = (Xevt_train_z-evt_mean)/evt_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_z = model.predict([X_train_z, Xalt_train_z, Xevt_train_z])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressed_z = (predictions_z * std_Y) + mean_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(regressed_z,bins=np.arange(70,200,1));\n",
    "plt.axvline(x=91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_Y = np.mean(y_train[:])\n",
    "std_Y = np.std(y_train[:])\n",
    "y_train[:] = (y_train[:]-mean_Y)/std_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, np.arange(-3,3,0.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Xalt_val, Xalt_test, Xevt_val, Xevt_test, y_val, y_test = train_test_split(X_test_htt, Xalt_test_htt, Xevt_test_htt, Y_test_htt, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val[:] = (y_val[:]-mean_Y)/std_Y\n",
    "y_test[:] = (y_test[:]-mean_Y)/std_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_val,np.arange(-1,1,0.01));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "N_EPOCHS = 80\n",
    "PRINT_INTERVAL = 2000\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleGru()\n",
    "\n",
    "print(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gruTrainData(torch.FloatTensor(X_train.astype(np.float)),torch.FloatTensor(Xalt_train.astype(np.float)),torch.FloatTensor(Xevt_train.astype(np.float)),torch.FloatTensor(y_train.astype(np.float)))\n",
    "val_data = gruTrainData(torch.FloatTensor(X_val.astype(np.float)),torch.FloatTensor(Xalt_val.astype(np.float)),torch.FloatTensor(Xevt_val.astype(np.float)),torch.FloatTensor(np.array(y_val).astype(np.float)))\n",
    "test_data = gruTestData(torch.FloatTensor(X_test.astype(np.float)),torch.FloatTensor(Xalt_test.astype(np.float)),torch.FloatTensor(Xevt_test.astype(np.float))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = utils.DataLoader(dataset=val_data, batch_size=100)\n",
    "test_loader = utils.DataLoader(dataset=test_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for x1,x2,x3, y in train_loader:\n",
    "        # reshape the data into [batch_size, 784]\n",
    "        x1,x2,x3,y = x1.to(device),x2.to(device),x3.to(device), y.to(device)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(x1,x2,x3)\n",
    "\n",
    "        # reconstruction loss\n",
    "        #recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "        # kl divergence loss\n",
    "        #kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "        # total loss\n",
    "        #loss = recon_loss + kl_loss\n",
    "\n",
    "        #BCE loss\n",
    "        loss = criterion(y_pred, y.unsqueeze(1))\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # test loss for the data\n",
    "    test_loss = 0\n",
    "\n",
    "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "    with torch.no_grad():\n",
    "        for x1,x2,x3, y in val_loader:\n",
    "            # reshape the data\n",
    "            #x = x.view(-1, 28 * 28)\n",
    "            x1,x2,x3,y = x1.to(device),x2.to(device),x3.to(device), y.to(device)\n",
    "            # forward pass\n",
    "            y_pred = model(x1,x2,x3)\n",
    "\n",
    "            loss = criterion(y_pred, y.unsqueeze(1))\n",
    "            # total loss\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"/data/t3home000/spark/MassRegression/weights/supervised_gru_allinput_{mixratio}_v0.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_loss = 9\n",
    "patience_counter = 0\n",
    "for e in range(500):\n",
    "\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "\n",
    "    train_loss /= len(train_data)\n",
    "    test_loss /= len(val_data)\n",
    "\n",
    "    print(f'Epoch {e}, Train Loss: {train_loss:.7f}, Test Loss: {test_loss:.7f}')\n",
    "\n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), f\"/data/t3home000/spark/MassRegression/weights/supervised_gru_allinput_{mixratio}_v0.h5\")\n",
    "        patience_counter = 1\n",
    "        print('saving model')\n",
    "    else:\n",
    "        print('NOT SAVING')\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > 10:\n",
    "        print('patience limit reached')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = np.array([],dtype=np.float)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x1,x2,x3) in enumerate(test_loader):\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        x3 = x3.to(device)\n",
    "        y_test_pred = model(x1,x2,x3)\n",
    "        #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        #y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list = np.append(y_pred_list,y_test_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mass = (y_pred_list * std_Y) + mean_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,b,patches = plt.hist(predicted_mass,np.arange(60,300,3))\n",
    "plt.text(200, 1000, f'mean: {np.mean(predicted_mass):.2f}', fontsize=15)\n",
    "plt.text(200, 800, f'std: {np.std(predicted_mass):.2f}', fontsize=15)\n",
    "\n",
    "plt.text(200, 600, f'mixratio: {mixratio}%', fontsize=15)\n",
    "\n",
    "plt.text(65, 1200, f'm={b[bin_max][0]}', fontsize=13)\n",
    "\n",
    "plt.axvline(x=b[bin_max][0],color='r')\n",
    "plt.savefig('hadhad_mixratio_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_max = np.where(n == n.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[bin_max][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relerr = (predicted_mass - 125) /125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(relerr,np.arange(-1.5,1.5,0.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IN_torch",
   "language": "python",
   "name": "in_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
